{
  "id": "arxiv:2602.03784",
  "title": "Context Compression via Explicit Information Transmission",
  "authors": [
    "Jiangnan Ye",
    "Hanqi Yan",
    "Zhenyi Shen",
    "Heng Chang",
    "Ye Mao",
    "Yulan He"
  ],
  "url": "https://huggingface.co/papers/2602.03784",
  "abstract": "Long-context inference with Large Language Models (LLMs) is costly due to quadratic attention and growing key-value caches, motivating context compression. In this work, we study soft context compression, where a long context is condensed into a small set of continuous representations. Existing methods typically re-purpose the LLM itself as a trainable compressor, relying on layer-by-layer self-attention to iteratively aggregate information. We argue that this paradigm suffers from two structural limitations: (i) progressive representation overwriting across layers (ii) uncoordinated allocation of compression capacity across tokens. We propose ComprExIT (Context Compression via Explicit Information Transmission), a lightweight framework that formulates soft compression into a new paradigm: explicit information transmission over frozen LLM hidden states. This decouples compression from the model's internal self-attention dynamics. ComprExIT performs (i) depth-wise transmission to selectively transmit multi-layer information into token anchors, mitigating progressive overwriting, and (ii) width-wise transmission to aggregate anchors into a small number of slots via a globally optimized transmission plan, ensuring coordinated allocation of information. Across six question-answering benchmarks, ComprExIT consistently outperforms state-of-the-art context compression methods while introducing only ~1% additional parameters, demonstrating that explicit and coordinated information transmission enables more effective and robust long-context compression.",
  "discovered_by": "hf_daily_papers",
  "discovered_date": "2026-02-10",
  "status": "discovered",
  "insights": [],
  "_meta": {
    "hf_upvotes": 14,
    "ai_keywords": [
      "soft context compression",
      "LLMs",
      "quadratic attention",
      "key-value caches",
      "layer-by-layer self-attention",
      "information transmission",
      "frozen LLM hidden states",
      "depth-wise transmission",
      "width-wise transmission",
      "token anchors",
      "compression capacity"
    ]
  }
}