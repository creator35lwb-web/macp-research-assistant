{
  "id": "arxiv:2602.06422",
  "title": "Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO",
  "authors": [
    "Yunze Tong",
    "Mushui Liu",
    "Canyu Zhao",
    "Wanggui He",
    "Shiyi Zhang",
    "Hongwei Zhang",
    "Peng Zhang",
    "Jinlong Liu",
    "Ju Huang",
    "Jiamang Wang",
    "Hao Jiang",
    "Pipei Huang"
  ],
  "url": "https://huggingface.co/papers/2602.06422",
  "abstract": "Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's \"pure\" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.",
  "discovered_by": "hf_daily_papers",
  "discovered_date": "2026-02-10",
  "status": "discovered",
  "insights": [],
  "_meta": {
    "hf_upvotes": 42,
    "github_repo": "https://github.com/YunzeTong/TurningPoint-GRPO",
    "ai_keywords": [
      "GRPO",
      "flow matching models",
      "text-to-image generation",
      "denoising steps",
      "reward sparsity",
      "incremental rewards",
      "turning points",
      "denoising trajectory",
      "delayed impact",
      "reward evolution"
    ]
  }
}