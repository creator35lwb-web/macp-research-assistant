{
  "id": "arxiv:2602.08629",
  "title": "CauScale: Neural Causal Discovery at Scale",
  "authors": [
    "Bo Peng",
    "Sirui Chen",
    "Jiaguo Tian",
    "Yu Qiao",
    "Chaochao Lu"
  ],
  "url": "https://huggingface.co/papers/2602.08629",
  "abstract": "Causal discovery is essential for advancing data-driven fields such as scientific AI and data analysis, yet existing approaches face significant time- and space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, a neural architecture designed for efficient causal discovery that scales inference to graphs with up to 1000 nodes. CauScale improves time efficiency via a reduction unit that compresses data embeddings and improves space efficiency by adopting tied attention weights to avoid maintaining axis-specific attention maps. To keep high causal discovery accuracy, CauScale adopts a two-stream design: a data stream extracts relational evidence from high-dimensional observations, while a graph stream integrates statistical graph priors and preserves key structural signals. CauScale successfully scales to 500-node graphs during training, where prior work fails due to space limitations. Across testing data with varying graph scales and causal mechanisms, CauScale achieves 99.6% mAP on in-distribution data and 84.4% on out-of-distribution data, while delivering 4-13,000 times inference speedups over prior methods. Our project page is at https://github.com/OpenCausaLab/CauScale.",
  "discovered_by": "hf_daily_papers",
  "discovered_date": "2026-02-10",
  "status": "discovered",
  "insights": [],
  "_meta": {
    "hf_upvotes": 1,
    "ai_keywords": [
      "causal discovery",
      "neural architecture",
      "data embeddings",
      "tied attention weights",
      "two-stream design",
      "relational evidence",
      "statistical graph priors",
      "structural signals",
      "mAP",
      "inference speedups"
    ]
  }
}