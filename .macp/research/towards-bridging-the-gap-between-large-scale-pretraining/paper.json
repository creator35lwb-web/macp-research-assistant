{
  "id": "arxiv:2601.21363",
  "title": "Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control",
  "authors": [
    "Weidong Huang",
    "Zhehan Li",
    "Hangxin Liu",
    "Biao Hou",
    "Yao Su",
    "Jingwen Zhang"
  ],
  "url": "https://huggingface.co/papers/2601.21363",
  "abstract": "Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.",
  "discovered_by": "hf_daily_papers",
  "discovered_date": "2026-02-10",
  "status": "discovered",
  "insights": [],
  "_meta": {
    "hf_upvotes": 4,
    "github_repo": "https://github.com/bigai-ai/LIFT-humanoid",
    "ai_keywords": [
      "Proximal Policy Optimization",
      "Soft Actor-Critic",
      "on-policy methods",
      "off-policy RL",
      "model-based RL",
      "large-scale parallel simulation",
      "zero-shot deployment",
      "sample efficiency",
      "large-batch update",
      "Update-To-Data ratio",
      "deterministic policy",
      "stochastic exploration",
      "physics-informed world model"
    ]
  }
}