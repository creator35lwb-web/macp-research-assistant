{
  "papers": [
    {
      "id": "arxiv:2502.09151",
      "title": "Regularization can make diffusion models more efficient",
      "authors": [
        "Mahsa Taheri",
        "Johannes Lederer"
      ],
      "url": "https://arxiv.org/abs/2502.09151",
      "abstract": "Diffusion models are one of the key architectures of generative AI. Their main drawback, however, is the computational costs. This study indicates that the concept of sparsity, well known especially in statistics, can provide a pathway to more efficient diffusion pipelines. Our mathematical guarantees prove that sparsity can reduce the input dimension's influence on the computational complexity to that of a much smaller intrinsic dimension of the data. Our empirical findings confirm that inducing sparsity can indeed lead to better samples at a lower cost.",
      "discovered_by": "arxiv_api",
      "discovered_date": "2025-02-13",
      "status": "discovered",
      "insights": []
    },
    {
      "id": "arxiv:2602.08222",
      "title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger",
      "authors": [
        "Zehao Chen",
        "Gongxun Li",
        "Tianxiang Ai",
        "Yifei Li",
        "Zixuan Huang",
        "Wang Zhou",
        "Fuzhen Zhuang",
        "Xianglong Liu",
        "Jianxin Li",
        "Deqing Wang",
        "Yikun Ban"
      ],
      "url": "https://huggingface.co/papers/2602.08222",
      "abstract": "As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "cited",
      "insights": [
        "Models experience saturation bottlenecks in post-training when they become overly confident, leading to diminishing returns from continued training",
        "Historical weak checkpoints of a model contain valuable information about recoverable learning gaps that can guide further optimization",
        "Using entropy dynamics to identify where weak and strong model versions differ enables targeted compensatory learning",
        "The approach achieves performance improvements on mathematical reasoning and code generation tasks without increasing inference costs"
      ],
      "_meta": {
        "hf_upvotes": 253,
        "github_repo": "https://github.com/chenzehao82/Weak-Driven-Learning",
        "ai_keywords": [
          "post-training optimization",
          "large language models",
          "saturation bottleneck",
          "weak checkpoints",
          "entropy dynamics",
          "compensatory learning",
          "learning gaps"
        ]
      }
    },
    {
      "id": "arxiv:2602.07274",
      "title": "TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents",
      "authors": [
        "Kaijie Zhu",
        "Yuzhou Nie",
        "Yijiang Li",
        "Yiming Huang",
        "Jialian Wu",
        "Jiang Liu",
        "Ximeng Sun",
        "Zhenfei Yin",
        "Lun Wang",
        "Zicheng Liu",
        "Emad Barsoum",
        "William Yang Wang",
        "Wenbo Guo"
      ],
      "url": "https://huggingface.co/papers/2602.07274",
      "abstract": "Executing complex terminal tasks remains a significant challenge for open-weight LLMs, constrained by two fundamental limitations. First, high-fidelity, executable training environments are scarce: environments synthesized from real-world repositories are not diverse and scalable, while trajectories synthesized by LLMs suffer from hallucinations. Second, standard instruction tuning uses expert trajectories that rarely exhibit simple mistakes common to smaller models. This creates a distributional mismatch, leaving student models ill-equipped to recover from their own runtime failures. To bridge these gaps, we introduce TermiGen, an end-to-end pipeline for synthesizing verifiable environments and resilient expert trajectories. Termi-Gen first generates functionally valid tasks and Docker containers via an iterative multi-agent refinement loop. Subsequently, we employ a Generator-Critic protocol that actively injects errors during trajectory collection, synthesizing data rich in error-correction cycles. Fine-tuned on this TermiGen-generated dataset, our TermiGen-Qwen2.5-Coder-32B achieves a 31.3% pass rate on TerminalBench. This establishes a new open-weights state-of-the-art, outperforming existing baselines and notably surpassing capable proprietary models such as o4-mini. Dataset is avaiable at https://github.com/ucsb-mlsec/terminal-bench-env.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 196,
        "github_repo": "https://github.com/ucsb-mlsec/terminal-bench-env",
        "ai_keywords": [
          "instruction tuning",
          "expert trajectories",
          "distributional mismatch",
          "multi-agent refinement loop",
          "Generator-Critic protocol",
          "error-correction cycles",
          "TerminalBench",
          "pass rate"
        ]
      }
    },
    {
      "id": "arxiv:2602.07085",
      "title": "QuantaAlpha: An Evolutionary Framework for LLM-Driven Alpha Mining",
      "authors": [
        "Jun Han",
        "Shuo Zhang",
        "Wei Li",
        "Zhi Yang",
        "Yifan Dong",
        "Tu Hu",
        "Jialuo Yuan",
        "Xiaomin Yu",
        "Yumo Zhu",
        "Fangqi Lou",
        "Xin Guo",
        "Zhaowei Liu",
        "Tianyi Jiang",
        "Ruichuan An",
        "Jingping Liu",
        "Biao Wu",
        "Rongze Chen",
        "Kunyi Wang",
        "Yifan Wang",
        "Sen Hu",
        "Xinbing Kong",
        "Liwen Zhang",
        "Ronghao Chen",
        "Huacan Wang"
      ],
      "url": "https://huggingface.co/papers/2602.07085",
      "abstract": "Financial markets are noisy and non-stationary, making alpha mining highly sensitive to noise in backtesting results and sudden market regime shifts. While recent agentic frameworks improve alpha mining automation, they often lack controllable multi-round search and reliable reuse of validated experience. To address these challenges, we propose QuantaAlpha, an evolutionary alpha mining framework that treats each end-to-end mining run as a trajectory and improves factors through trajectory-level mutation and crossover operations. QuantaAlpha localizes suboptimal steps in each trajectory for targeted revision and recombines complementary high-reward segments to reuse effective patterns, enabling structured exploration and refinement across mining iterations. During factor generation, QuantaAlpha enforces semantic consistency across the hypothesis, factor expression, and executable code, while constraining the complexity and redundancy of the generated factor to mitigate crowding. Extensive experiments on the China Securities Index 300 (CSI 300) demonstrate consistent gains over strong baseline models and prior agentic systems. When utilizing GPT-5.2, QuantaAlpha achieves an Information Coefficient (IC) of 0.1501, with an Annualized Rate of Return (ARR) of 27.75% and a Maximum Drawdown (MDD) of 7.98%. Moreover, factors mined on CSI 300 transfer effectively to the China Securities Index 500 (CSI 500) and the Standard & Poor's 500 Index (S&P 500), delivering 160% and 137% cumulative excess return over four years, respectively, which indicates strong robustness of QuantaAlpha under market distribution shifts.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 180,
        "github_repo": "https://github.com/QuantaAlpha/QuantaAlpha"
      }
    },
    {
      "id": "arxiv:2602.08794",
      "title": "MOVA: Towards Scalable and Synchronized Video-Audio Generation",
      "authors": [
        "SII-OpenMOSS Team",
        "Donghua Yu",
        "Mingshu Chen",
        "Qi Chen",
        "Qi Luo",
        "Qianyi Wu",
        "Qinyuan Cheng",
        "Ruixiao Li",
        "Tianyi Liang",
        "Wenbo Zhang",
        "Wenming Tu",
        "Xiangyu Peng",
        "Yang Gao",
        "Yanru Huo",
        "Ying Zhu",
        "Yinze Luo",
        "Yiyang Zhang",
        "Yuerong Song",
        "Zhe Xu",
        "Zhiyu Zhang",
        "Chenchen Yang",
        "Cheng Chang",
        "Chushu Zhou",
        "Hanfu Chen",
        "Hongnan Ma",
        "Jiaxi Li",
        "Jingqi Tong",
        "Junxi Liu",
        "Ke Chen",
        "Shimin Li",
        "Songlin Wang",
        "Wei Jiang",
        "Zhaoye Fei",
        "Zhiyuan Ning",
        "Chunguo Li",
        "Chenhui Li",
        "Ziwei He",
        "Zengfeng Huang",
        "Xie Chen",
        "Xipeng Qiu"
      ],
      "url": "https://huggingface.co/papers/2602.08794",
      "abstract": "Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 150,
        "github_repo": "https://github.com/OpenMOSS/MOVA",
        "ai_keywords": [
          "Mixture-of-Experts",
          "MoE",
          "audio-visual content",
          "lip-synced speech",
          "sound effects",
          "content-aligned music",
          "IT2VA",
          "efficient inference",
          "LoRA fine-tuning",
          "prompt enhancement"
        ]
      }
    },
    {
      "id": "arxiv:2602.07026",
      "title": "Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models",
      "authors": [
        "Xiaomin Yu",
        "Yi Xin",
        "Wenjie Zhang",
        "Chonghan Liu",
        "Hanzhen Zhao",
        "Xiaoxing Hu",
        "Xinlei Yu",
        "Ziyue Qiao",
        "Hao Tang",
        "Xue Yang",
        "Xiaobin Hu",
        "Chengwei Qin",
        "Hui Xiong",
        "Yu Qiao",
        "Shuicheng Yan"
      ],
      "url": "https://huggingface.co/papers/2602.07026",
      "abstract": "Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 133,
        "github_repo": "https://github.com/Yu-xm/ReVision",
        "ai_keywords": [
          "multimodal contrastive learning",
          "modality gap",
          "geometric anomaly",
          "isotropic assumptions",
          "Fixed-frame Modality Gap Theory",
          "ReAlign",
          "Anchor Alignment",
          "Trace Alignment",
          "Centroid Alignment",
          "ReVision",
          "Multimodal Large Language Models",
          "unpaired data",
          "visual representation distribution",
          "pretraining stage",
          "visual instruction tuning"
        ]
      }
    },
    {
      "id": "arxiv:2602.06855",
      "title": "AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents",
      "authors": [
        "Alisia Lupidi",
        "Bhavul Gauri",
        "Thomas Simon Foster",
        "Bassel Al Omari",
        "Despoina Magka",
        "Alberto Pepe",
        "Alexis Audran-Reiss",
        "Muna Aghamelu",
        "Nicolas Baldwin",
        "Lucia Cipolina-Kun",
        "Jean-Christophe Gagnon-Audet",
        "Chee Hau Leow",
        "Sandra Lefdal",
        "Hossam Mossalam",
        "Abhinav Moudgil",
        "Saba Nazir",
        "Emanuel Tewolde",
        "Isabel Urrego",
        "Jordi Armengol Estape",
        "Amar Budhiraja",
        "Gaurav Chaurasia",
        "Abhishek Charnalia",
        "Derek Dunfield",
        "Karen Hambardzumyan",
        "Daniel Izcovich",
        "Martin Josifoski",
        "Ishita Mediratta",
        "Kelvin Niu",
        "Parth Pathak",
        "Michael Shvartsman",
        "Edan Toledo",
        "Anton Protopopov",
        "Roberta Raileanu",
        "Alexander Miller",
        "Tatiana Shavrina",
        "Jakob Foerster",
        "Yoram Bachrach"
      ],
      "url": "https://huggingface.co/papers/2602.06855",
      "abstract": "LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilities over the full research lifecycle -- including idea generation, experiment analysis and iterative refinement -- without providing baseline code. The AIRS-Bench task format is versatile, enabling easy integration of new tasks and rigorous comparison across different agentic frameworks. We establish baselines using frontier models paired with both sequential and parallel scaffolds. Our results show that agents exceed human SOTA in four tasks but fail to match it in sixteen others. Even when agents surpass human benchmarks, they do not reach the theoretical performance ceiling for the underlying tasks. These findings indicate that AIRS-Bench is far from saturated and offers substantial room for improvement. We open-source the AIRS-Bench task definitions and evaluation code to catalyze further development in autonomous scientific research.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 70,
        "github_repo": "https://github.com/facebookresearch/airs-bench",
        "ai_keywords": [
          "LLM agents",
          "AI Research Science Benchmark",
          "agentic capabilities",
          "research lifecycle",
          "sequential scaffolds",
          "parallel scaffolds",
          "human SOTA",
          "theoretical performance ceiling"
        ]
      }
    },
    {
      "id": "arxiv:2602.08990",
      "title": "InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery",
      "authors": [
        "Shiyang Feng",
        "Runmin Ma",
        "Xiangchao Yan",
        "Yue Fan",
        "Yusong Hu",
        "Songtao Huang",
        "Shuaiyu Zhang",
        "Zongsheng Cao",
        "Tianshuo Peng",
        "Jiakang Yuan",
        "Zijie Guo",
        "Zhijie Zhong",
        "Shangheng Du",
        "Weida Wang",
        "Jinxin Shi",
        "Yuhao Zhou",
        "Xiaohan He",
        "Zhiyin Yu",
        "Fangchen Yu",
        "Qihao Zheng",
        "Jiamin Wu",
        "Mianxin Liu",
        "Chi Zhang",
        "Shaowei Hou",
        "Shuya Li",
        "Yankai Jiang",
        "Wenjie Lou",
        "Lilong Wang",
        "Zifu Wang",
        "Jiong Wang",
        "Wanghan Xu",
        "Yue Deng",
        "Dongrui Liu",
        "Yiheng Wang",
        "Wenlong Zhang",
        "Fenghua Ling",
        "Shufei Zhang",
        "Xiaosong Wang",
        "Shuangjia Zheng",
        "Xun Huang",
        "Siqi Sun",
        "Shuyue Hu",
        "Peng Ye",
        "Chunfeng Song",
        "Bin Wang",
        "Conghui He",
        "Yihao Liu",
        "Xin Li",
        "Qibin Hou",
        "Tao Chen",
        "Xiangyu Yue",
        "Bin Wang",
        "Liang He",
        "Dahua Lin",
        "Bowen Zhou",
        "Bo Zhang",
        "Lei Bai"
      ],
      "url": "https://huggingface.co/papers/2602.08990",
      "abstract": "We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 68,
        "github_repo": "https://github.com/InternScience/InternAgent",
        "ai_keywords": [
          "scientific discovery",
          "computational modeling",
          "laboratory experimentation",
          "unified system",
          "deep research",
          "solution optimization",
          "long horizon memory",
          "scientific reasoning benchmarks",
          "algorithm discovery",
          "empirical discovery"
        ]
      }
    },
    {
      "id": "arxiv:2602.07845",
      "title": "Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning",
      "authors": [
        "Yalcin Tur",
        "Jalal Naghiyev",
        "Haoquan Fang",
        "Wei-Chuan Tsai",
        "Jiafei Duan",
        "Dieter Fox",
        "Ranjay Krishna"
      ],
      "url": "https://huggingface.co/papers/2602.07845",
      "abstract": "Current Vision-Language-Action (VLA) models rely on fixed computational depth, expending the same amount of compute on simple adjustments and complex multi-step manipulation. While Chain-of-Thought (CoT) prompting enables variable computation, it scales memory linearly and is ill-suited for continuous action spaces. We introduce Recurrent-Depth VLA (RD-VLA), an architecture that achieves computational adaptivity via latent iterative refinement rather than explicit token generation. RD-VLA employs a recurrent, weight-tied action head that supports arbitrary inference depth with a constant memory footprint. The model is trained using truncated backpropagation through time (TBPTT) to efficiently supervise the refinement process. At inference, RD-VLA dynamically allocates compute using an adaptive stopping criterion based on latent convergence. Experiments on challenging manipulation tasks show that recurrent depth is critical: tasks that fail entirely (0 percent success) with single-iteration inference exceed 90 percent success with four iterations, while simpler tasks saturate rapidly. RD-VLA provides a scalable path to test-time compute in robotics, replacing token-based reasoning with latent reasoning to achieve constant memory usage and up to 80x inference speedup over prior reasoning-based VLA models. Project page: https://rd-vla.github.io/",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 67,
        "github_repo": "https://github.com/rd-vla/rd-vla",
        "ai_keywords": [
          "Vision-Language-Action models",
          "Chain-of-Thought prompting",
          "recurrent architecture",
          "weight-tied action head",
          "truncated backpropagation through time",
          "latent iterative refinement",
          "adaptive stopping criterion",
          "latent convergence",
          "computational adaptivity"
        ]
      }
    },
    {
      "id": "arxiv:2602.08676",
      "title": "LLaDA2.1: Speeding Up Text Diffusion via Token Editing",
      "authors": [
        "Tiwei Bie",
        "Maosong Cao",
        "Xiang Cao",
        "Bingsen Chen",
        "Fuyuan Chen",
        "Kun Chen",
        "Lun Du",
        "Daozhuo Feng",
        "Haibo Feng",
        "Mingliang Gong",
        "Zhuocheng Gong",
        "Yanmei Gu",
        "Jian Guan",
        "Kaiyuan Guan",
        "Hongliang He",
        "Zenan Huang",
        "Juyong Jiang",
        "Zhonghui Jiang",
        "Zhenzhong Lan",
        "Chengxi Li",
        "Jianguo Li",
        "Zehuan Li",
        "Huabin Liu",
        "Lin Liu",
        "Guoshan Lu",
        "Yuan Lu",
        "Yuxin Ma",
        "Xingyu Mou",
        "Zhenxuan Pan",
        "Kaida Qiu",
        "Yuji Ren",
        "Jianfeng Tan",
        "Yiding Tian",
        "Zian Wang",
        "Lanning Wei",
        "Tao Wu",
        "Yipeng Xing",
        "Wentao Ye",
        "Liangyu Zha",
        "Tianze Zhang",
        "Xiaolu Zhang",
        "Junbo Zhao",
        "Da Zheng",
        "Hao Zhong",
        "Wanli Zhong",
        "Jun Zhou",
        "Junlin Zhou",
        "Liwang Zhu",
        "Muzhi Zhu",
        "Yihong Zhuang"
      ],
      "url": "https://huggingface.co/papers/2602.08676",
      "abstract": "While LLaDA2.0 showcased the scaling potential of 100B-level block-diffusion models and their inherent parallelization, the delicate equilibrium between decoding speed and generation quality has remained an elusive frontier. Today, we unveil LLaDA2.1, a paradigm shift designed to transcend this trade-off. By seamlessly weaving Token-to-Token (T2T) editing into the conventional Mask-to-Token (M2T) scheme, we introduce a joint, configurable threshold-decoding scheme. This structural innovation gives rise to two distinct personas: the Speedy Mode (S Mode), which audaciously lowers the M2T threshold to bypass traditional constraints while relying on T2T to refine the output; and the Quality Mode (Q Mode), which leans into conservative thresholds to secure superior benchmark performances with manageable efficiency degrade. Furthering this evolution, underpinned by an expansive context window, we implement the first large-scale Reinforcement Learning (RL) framework specifically tailored for dLLMs, anchored by specialized techniques for stable gradient estimation. This alignment not only sharpens reasoning precision but also elevates instruction-following fidelity, bridging the chasm between diffusion dynamics and complex human intent. We culminate this work by releasing LLaDA2.1-Mini (16B) and LLaDA2.1-Flash (100B). Across 33 rigorous benchmarks, LLaDA2.1 delivers strong task performance and lightning-fast decoding speed. Despite its 100B volume, on coding tasks it attains an astounding 892 TPS on HumanEval+, 801 TPS on BigCodeBench, and 663 TPS on LiveCodeBench.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 65,
        "github_repo": "https://github.com/inclusionAI/LLaDA2.X",
        "ai_keywords": [
          "block-diffusion models",
          "decoding speed",
          "generation quality",
          "Token-to-Token editing",
          "Mask-to-Token scheme",
          "threshold-decoding scheme",
          "Speedy Mode",
          "Quality Mode",
          "Reinforcement Learning",
          "gradient estimation",
          "reasoning precision",
          "instruction-following",
          "large language diffusion models",
          "HumanEval+",
          "BigCodeBench",
          "LiveCodeBench"
        ]
      }
    },
    {
      "id": "arxiv:2602.07837",
      "title": "RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI",
      "authors": [
        "Hongzhi Zang",
        "Shu'ang Yu",
        "Hao Lin",
        "Tianxing Zhou",
        "Zefang Huang",
        "Zhen Guo",
        "Xin Xu",
        "Jiakai Zhou",
        "Yuze Sheng",
        "Shizhe Zhang",
        "Feng Gao",
        "Wenhao Tang",
        "Yufeng Yue",
        "Quanlu Zhang",
        "Xinlei Chen",
        "Chao Yu",
        "Yu Wang"
      ],
      "url": "https://huggingface.co/papers/2602.07837",
      "abstract": "Online policy learning directly in the physical world is a promising yet challenging direction for embodied intelligence. Unlike simulation, real-world systems cannot be arbitrarily accelerated, cheaply reset, or massively replicated, which makes scalable data collection, heterogeneous deployment, and long-horizon effective training difficult. These challenges suggest that real-world policy learning is not only an algorithmic issue but fundamentally a systems problem. We present USER, a Unified and extensible SystEm for Real-world online policy learning. USER treats physical robots as first-class hardware resources alongside GPUs through a unified hardware abstraction layer, enabling automatic discovery, management, and scheduling of heterogeneous robots. To address cloud-edge communication, USER introduces an adaptive communication plane with tunneling-based networking, distributed data channels for traffic localization, and streaming-multiprocessor-aware weight synchronization to regulate GPU-side overhead. On top of this infrastructure, USER organizes learning as a fully asynchronous framework with a persistent, cache-aware buffer, enabling efficient long-horizon experiments with robust crash recovery and reuse of historical data. In addition, USER provides extensible abstractions for rewards, algorithms, and policies, supporting online imitation or reinforcement learning of CNN/MLP, generative policies, and large vision-language-action (VLA) models within a unified pipeline. Results in both simulation and the real world show that USER enables multi-robot coordination, heterogeneous manipulators, edge-cloud collaboration with large models, and long-running asynchronous training, offering a unified and extensible systems foundation for real-world online policy learning.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 53,
        "ai_keywords": [
          "online policy learning",
          "embodied intelligence",
          "real-world systems",
          "heterogeneous robots",
          "hardware abstraction layer",
          "adaptive communication plane",
          "tunneling-based networking",
          "distributed data channels",
          "streaming-multiprocessor-aware weight synchronization",
          "asynchronous framework",
          "cache-aware buffer",
          "crash recovery",
          "reinforcement learning",
          "imitation learning",
          "vision-language-action models",
          "multi-robot coordination",
          "edge-cloud collaboration"
        ]
      }
    },
    {
      "id": "arxiv:2602.00169",
      "title": "Towards Agentic Intelligence for Materials Science",
      "authors": [
        "Huan Zhang",
        "Yizhan Li",
        "Wenhao Huang",
        "Ziyu Hou",
        "Yu Song",
        "Xuye Liu",
        "Farshid Effaty",
        "Jinya Jiang",
        "Sifan Wu",
        "Qianggang Ding",
        "Izumi Takahara",
        "Leonard R. MacGillivray",
        "Teruyasu Mizoguchi",
        "Tianshu Yu",
        "Lizi Liao",
        "Yuyu Luo",
        "Yu Rong",
        "Jia Li",
        "Ying Diao",
        "Heng Ji",
        "Bang Liu"
      ],
      "url": "https://huggingface.co/papers/2602.00169",
      "abstract": "The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment.\n  To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 45,
        "ai_keywords": [
          "artificial intelligence",
          "materials science",
          "agentic systems",
          "pipeline-centric view",
          "domain adaptation",
          "instruction tuning",
          "goal-conditioned agents",
          "simulation platforms",
          "experimental platforms",
          "credit assignment",
          "LLMs",
          "pattern recognition",
          "predictive analytics",
          "natural language processing",
          "literature mining",
          "materials characterization",
          "property prediction",
          "materials design",
          "process optimization",
          "computational workflows",
          "DFT",
          "robotic labs",
          "passive approaches",
          "reactive approaches",
          "autonomous agents",
          "safety-aware agents"
        ]
      }
    },
    {
      "id": "arxiv:2602.06422",
      "title": "Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO",
      "authors": [
        "Yunze Tong",
        "Mushui Liu",
        "Canyu Zhao",
        "Wanggui He",
        "Shiyi Zhang",
        "Hongwei Zhang",
        "Peng Zhang",
        "Jinlong Liu",
        "Ju Huang",
        "Jiamang Wang",
        "Hao Jiang",
        "Pipei Huang"
      ],
      "url": "https://huggingface.co/papers/2602.06422",
      "abstract": "Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's \"pure\" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 42,
        "github_repo": "https://github.com/YunzeTong/TurningPoint-GRPO",
        "ai_keywords": [
          "GRPO",
          "flow matching models",
          "text-to-image generation",
          "denoising steps",
          "reward sparsity",
          "incremental rewards",
          "turning points",
          "denoising trajectory",
          "delayed impact",
          "reward evolution"
        ]
      }
    },
    {
      "id": "arxiv:2602.08321",
      "title": "Improving Data and Reward Design for Scientific Reasoning in Large Language Models",
      "authors": [
        "Zijie Chen",
        "Zhenghao Lin",
        "Xiao Liu",
        "Zhenzhong Lan",
        "Yeyun Gong",
        "Peng Cheng"
      ],
      "url": "https://huggingface.co/papers/2602.08321",
      "abstract": "Solving open-ended science questions remains challenging for large language models, particularly due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset, which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT, which broadens the model's reasoning pattern coverage prior to RL; (ii) Dynamic Difficulty Curriculum, which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL, which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using Dr. SCI pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general, consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning, especially in open-ended settings.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 39,
        "ai_keywords": [
          "large language models",
          "open-ended science questions",
          "data construction",
          "reward design",
          "Dr. SCI dataset",
          "SFT",
          "RL",
          "exploration-expanding SFT",
          "dynamic difficulty curriculum",
          "SciRubric-Guided RL",
          "scientific reasoning",
          "GPQA-diamond",
          "GPQA-general"
        ]
      }
    },
    {
      "id": "arxiv:2602.09007",
      "title": "GEBench: Benchmarking Image Generation Models as GUI Environments",
      "authors": [
        "Haodong Li",
        "Jingwei Wu",
        "Quan Sun",
        "Guopeng Li",
        "Juanxi Tian",
        "Huanyu Zhang",
        "Yanlin Lai",
        "Ruichuan An",
        "Hongbo Peng",
        "Yuhong Dai",
        "Chenxi Li",
        "Chunmei Qing",
        "Jia Wang",
        "Ziyang Meng",
        "Zheng Ge",
        "Xiangyu Zhang",
        "Daxin Jiang"
      ],
      "url": "https://huggingface.co/papers/2602.09007",
      "abstract": "Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 38,
        "ai_keywords": [
          "GUI generation",
          "temporal coherence",
          "dynamic interaction",
          "visual fidelity",
          "GUI-specific contexts",
          "GEBench",
          "GE-Score",
          "goal achievement",
          "interaction logic",
          "content consistency",
          "UI plausibility",
          "visual quality"
        ]
      }
    },
    {
      "id": "arxiv:2602.08439",
      "title": "Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition",
      "authors": [
        "Yuhao Dong",
        "Shulin Tian",
        "Shuai Liu",
        "Shuangrui Ding",
        "Yuhang Zang",
        "Xiaoyi Dong",
        "Yuhang Cao",
        "Jiaqi Wang",
        "Ziwei Liu"
      ],
      "url": "https://huggingface.co/papers/2602.08439",
      "abstract": "Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 28,
        "github_repo": "https://github.com/dongyh20/Demo-ICL",
        "ai_keywords": [
          "Multimodal Large Language Models",
          "video understanding",
          "in-context learning",
          "video benchmarks",
          "Demo-ICL-Bench",
          "video-supervised fine-tuning",
          "direct preference optimization"
        ]
      }
    },
    {
      "id": "arxiv:2602.06025",
      "title": "Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory",
      "authors": [
        "Haozhen Zhang",
        "Haodong Yue",
        "Tao Feng",
        "Quanyu Long",
        "Jianzhu Bao",
        "Bowen Jin",
        "Weizhi Zhang",
        "Xiao Li",
        "Jiaxuan You",
        "Chengwei Qin",
        "Wenya Wang"
      ],
      "url": "https://huggingface.co/papers/2602.06025",
      "abstract": "Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present BudgetMem, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., Low/Mid/High). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 27,
        "github_repo": "https://github.com/ViktorAxelsen/BudgetMem",
        "ai_keywords": [
          "runtime memory utilization",
          "query-aware performance-cost control",
          "memory modules",
          "budget tiers",
          "lightweight router",
          "neural policy",
          "reinforcement learning",
          "LoCoMo",
          "LongMemEval",
          "HotpotQA"
        ]
      }
    },
    {
      "id": "arxiv:2602.08543",
      "title": "GISA: A Benchmark for General Information-Seeking Assistant",
      "authors": [
        "Yutao Zhu",
        "Xingshuo Zhang",
        "Maosen Zhang",
        "Jiajie Jin",
        "Liancheng Zhang",
        "Xiaoshuai Song",
        "Kangzhi Zhao",
        "Wencong Zeng",
        "Ruiming Tang",
        "Han Li",
        "Ji-Rong Wen",
        "Zhicheng Dou"
      ],
      "url": "https://huggingface.co/papers/2602.08543",
      "abstract": "The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 26,
        "github_repo": "https://github.com/RUC-NLPIR/GISA",
        "ai_keywords": [
          "large language models",
          "search agents",
          "information-seeking assistants",
          "benchmarks",
          "deep reasoning",
          "information aggregation",
          "exact match score",
          "complex planning"
        ]
      }
    },
    {
      "id": "arxiv:2602.07962",
      "title": "LOCA-bench: Benchmarking Language Agents Under Controllable and Extreme Context Growth",
      "authors": [
        "Weihao Zeng",
        "Yuzhen Huang",
        "Junxian He"
      ],
      "url": "https://huggingface.co/papers/2602.07962",
      "abstract": "Large language models (LLMs) are increasingly capable of carrying out long-running, real-world tasks. However, as the amount of context grows, their reliability often deteriorates, a phenomenon known as \"context rot\". Existing long-context benchmarks primarily focus on single-step settings that evaluate a model's ability to retrieve information from a long snippet. In realistic scenarios, however, LLMs often need to act as agents that explore environments, follow instructions and plans, extract useful information, and predict correct actions under a dynamically growing context. To assess language agents in such settings, we introduce LOCA-bench (a benchmark for LOng-Context Agents). Given a task prompt, LOCA-bench leverages automated and scalable control of environment states to regulate the agent's context length. This design enables LOCA-bench to extend the context length potentially to infinity in a controlled way while keeping the underlying task semantics fixed. LOCA-bench evaluates language agents as a combination of models and scaffolds, including various context management strategies. While agent performance generally degrades as the environment states grow more complex, advanced context management techniques can substantially improve the overall success rate. We open-source LOCA-bench to provide a platform for evaluating models and scaffolds in long-context, agentic scenarios: https://github.com/hkust-nlp/LOCA-bench",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 24,
        "github_repo": "https://github.com/hkust-nlp/LOCA-bench",
        "ai_keywords": [
          "large language models",
          "context rot",
          "long-context benchmarks",
          "language agents",
          "LOCA-bench",
          "environment states",
          "context management strategies",
          "agent performance"
        ]
      }
    },
    {
      "id": "arxiv:2602.06540",
      "title": "AgentCPM-Report: Interleaving Drafting and Deepening for Open-Ended Deep Research",
      "authors": [
        "Yishan Li",
        "Wentong Chen",
        "Yukun Yan",
        "Mingwei Li",
        "Sen Mei",
        "Xiaorong Wang",
        "Kunpeng Liu",
        "Xin Cong",
        "Shuo Wang",
        "Zhong Zhang",
        "Yaxi Lu",
        "Zhenghao Liu",
        "Yankai Lin",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "url": "https://huggingface.co/papers/2602.06540",
      "abstract": "Generating deep research reports requires large-scale information acquisition and the synthesis of insight-driven analysis, posing a significant challenge for current language models. Most existing approaches follow a plan-then-write paradigm, whose performance heavily depends on the quality of the initial outline. However, constructing a comprehensive outline itself demands strong reasoning ability, causing current deep research systems to rely almost exclusively on closed-source or online large models. This reliance raises practical barriers to deployment and introduces safety and privacy concerns for user-authored data. In this work, we present AgentCPM-Report, a lightweight yet high-performing local solution composed of a framework that mirrors the human writing process and an 8B-parameter deep research agent. Our framework uses a Writing As Reasoning Policy (WARP), which enables models to dynamically revise outlines during report generation. Under this policy, the agent alternates between Evidence-Based Drafting and Reasoning-Driven Deepening, jointly supporting information acquisition, knowledge refinement, and iterative outline evolution. To effectively equip small models with this capability, we introduce a Multi-Stage Agentic Training strategy, consisting of cold-start, atomic skill RL, and holistic pipeline RL. Experiments on DeepResearch Bench, DeepConsult, and DeepResearch Gym demonstrate that AgentCPM-Report outperforms leading closed-source systems, with substantial gains in Insight.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 21,
        "github_repo": "https://github.com/OpenBMB/AgentCPM",
        "ai_keywords": [
          "Writing As Reasoning Policy",
          "WARP",
          "Evidence-Based Drafting",
          "Reasoning-Driven Deepening",
          "Multi-Stage Agentic Training",
          "cold-start",
          "atomic skill RL",
          "holistic pipeline RL",
          "deep research agent",
          "insight-driven analysis",
          "plan-then-write paradigm"
        ]
      }
    },
    {
      "id": "arxiv:2602.07055",
      "title": "Theory of Space: Can Foundation Models Construct Spatial Beliefs through Active Exploration?",
      "authors": [
        "Pingyue Zhang",
        "Zihan Huang",
        "Yue Wang",
        "Jieyu Zhang",
        "Letian Xue",
        "Zihan Wang",
        "Qineng Wang",
        "Keshigeyan Chandrasegaran",
        "Ruohan Zhang",
        "Yejin Choi",
        "Ranjay Krishna",
        "Jiajun Wu",
        "Li Fei-Fei",
        "Manling Li"
      ],
      "url": "https://huggingface.co/papers/2602.07055",
      "abstract": "Spatial embodied intelligence requires agents to act to acquire information under partial observability. While multimodal foundation models excel at passive perception, their capacity for active, self-directed exploration remains understudied. We propose Theory of Space, defined as an agent's ability to actively acquire information through self-directed, active exploration and to construct, revise, and exploit a spatial belief from sequential, partial observations. We evaluate this through a benchmark where the goal is curiosity-driven exploration to build an accurate cognitive map. A key innovation is spatial belief probing, which prompts models to reveal their internal spatial representations at each step. Our evaluation of state-of-the-art models reveals several critical bottlenecks. First, we identify an Active-Passive Gap, where performance drops significantly when agents must autonomously gather information. Second, we find high inefficiency, as models explore unsystematically compared to program-based proxies. Through belief probing, we diagnose that while perception is an initial bottleneck, global beliefs suffer from instability that causes spatial knowledge to degrade over time. Finally, using a false belief paradigm, we uncover Belief Inertia, where agents fail to update obsolete priors with new evidence. This issue is present in text-based agents but is particularly severe in vision-based models. Our findings suggest that current foundation models struggle to maintain coherent, revisable spatial beliefs during active exploration.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 21,
        "github_repo": "https://github.com/mll-lab-nu/Theory-of-Space",
        "ai_keywords": [
          "spatial embodied intelligence",
          "multimodal foundation models",
          "active exploration",
          "spatial belief",
          "cognitive mapping",
          "spatial belief probing",
          "Active-Passive Gap",
          "belief inertia",
          "vision-based models",
          "text-based agents"
        ]
      }
    },
    {
      "id": "arxiv:2602.09022",
      "title": "WorldCompass: Reinforcement Learning for Long-Horizon World Models",
      "authors": [
        "Zehan Wang",
        "Tengfei Wang",
        "Haiyu Zhang",
        "Xuhui Zuo",
        "Junta Wu",
        "Haoyuan Wang",
        "Wenqiang Sun",
        "Zhenwei Wang",
        "Chenjie Cao",
        "Hengshuang Zhao",
        "Chunchao Guo",
        "Zhou Zhao"
      ],
      "url": "https://huggingface.co/papers/2602.09022",
      "abstract": "This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively \"steer\" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 20,
        "ai_keywords": [
          "Reinforcement Learning",
          "world models",
          "video generation",
          "rollout strategy",
          "reward functions",
          "reward-hacking",
          "negative-aware fine-tuning",
          "efficiency optimizations"
        ]
      }
    },
    {
      "id": "arxiv:2602.07075",
      "title": "LatentChem: From Textual CoT to Latent Thinking in Chemical Reasoning",
      "authors": [
        "Xinwu Ye",
        "Yicheng Mao",
        "Jia Zhang",
        "Yimeng Liu",
        "Li Hao",
        "Fang Wu",
        "Zhiwei Li",
        "Yuxuan Liao",
        "Zehong Wang",
        "Zhiyuan Liu",
        "Zhenfei Yin",
        "Li Yuan",
        "Philip Torr",
        "Huan Sun",
        "Xiangxiang Zeng",
        "Mengdi Wang",
        "Le Cong",
        "Shenghua Gao",
        "Xiangru Tang"
      ],
      "url": "https://huggingface.co/papers/2602.07075",
      "abstract": "Chemical large language models (LLMs) predominantly rely on explicit Chain-of-Thought (CoT) in natural language to perform complex reasoning. However, chemical reasoning is inherently continuous and structural, and forcing it into discrete linguistic tokens introduces a fundamental representation mismatch that constrains both efficiency and performance. We introduce LatentChem, a latent reasoning interface that decouples chemical computation from textual generation, enabling models to perform multi-step reasoning directly in continuous latent space while emitting language only for final outputs. Remarkably, we observe a consistent emergent behavior: when optimized solely for task success, models spontaneously internalize reasoning, progressively abandoning verbose textual derivations in favor of implicit latent computation. This shift is not merely stylistic but computationally advantageous. Across diverse chemical reasoning benchmarks, LatentChem achieves a 59.88\\% non-tie win rate over strong CoT-based baselines on ChemCoTBench, while delivering a 10.84times average inference speedup. Our results provide empirical evidence that chemical reasoning is more naturally and effectively realized as continuous latent dynamics rather than discretized linguistic trajectories.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 18,
        "github_repo": "https://github.com/xinwuye/LatentChem",
        "ai_keywords": [
          "chemical large language models",
          "Chain-of-Thought",
          "latent reasoning",
          "continuous latent space",
          "textual generation",
          "multi-step reasoning",
          "ChemCoTBench",
          "inference speedup"
        ]
      }
    },
    {
      "id": "arxiv:2602.06694",
      "title": "NanoQuant: Efficient Sub-1-Bit Quantization of Large Language Models",
      "authors": [
        "Hyochan Chong",
        "Dongkyu Kim",
        "Changdong Kim",
        "Minseop Choi"
      ],
      "url": "https://huggingface.co/papers/2602.06694",
      "abstract": "Weight-only quantization has become a standard approach for efficiently serving large language models (LLMs). However, existing methods fail to efficiently compress models to binary (1-bit) levels, as they either require large amounts of data and compute or incur additional storage. In this work, we propose NanoQuant, the first post-training quantization (PTQ) method to compress LLMs to both binary and sub-1-bit levels. NanoQuant formulates quantization as a low-rank binary factorization problem, and compresses full-precision weights to low-rank binary matrices and scales. Specifically, it utilizes an efficient alternating direction method of multipliers (ADMM) method to precisely initialize latent binary matrices and scales, and then tune the initialized parameters through a block and model reconstruction process. Consequently, NanoQuant establishes a new Pareto frontier in low-memory post-training quantization, achieving state-of-the-art accuracy even at sub-1-bit compression rates. NanoQuant makes large-scale deployment feasible on consumer hardware. For example, it compresses Llama2-70B by 25.8times in just 13 hours on a single H100, enabling a 70B model to operate on a consumer 8 GB GPU.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 15,
        "ai_keywords": [
          "post-training quantization",
          "low-rank binary factorization",
          "alternating direction method of multipliers",
          "binary quantization",
          "sub-1-bit compression",
          "latent binary matrices",
          "block reconstruction",
          "model reconstruction"
        ]
      }
    },
    {
      "id": "arxiv:2602.03784",
      "title": "Context Compression via Explicit Information Transmission",
      "authors": [
        "Jiangnan Ye",
        "Hanqi Yan",
        "Zhenyi Shen",
        "Heng Chang",
        "Ye Mao",
        "Yulan He"
      ],
      "url": "https://huggingface.co/papers/2602.03784",
      "abstract": "Long-context inference with Large Language Models (LLMs) is costly due to quadratic attention and growing key-value caches, motivating context compression. In this work, we study soft context compression, where a long context is condensed into a small set of continuous representations. Existing methods typically re-purpose the LLM itself as a trainable compressor, relying on layer-by-layer self-attention to iteratively aggregate information. We argue that this paradigm suffers from two structural limitations: (i) progressive representation overwriting across layers (ii) uncoordinated allocation of compression capacity across tokens. We propose ComprExIT (Context Compression via Explicit Information Transmission), a lightweight framework that formulates soft compression into a new paradigm: explicit information transmission over frozen LLM hidden states. This decouples compression from the model's internal self-attention dynamics. ComprExIT performs (i) depth-wise transmission to selectively transmit multi-layer information into token anchors, mitigating progressive overwriting, and (ii) width-wise transmission to aggregate anchors into a small number of slots via a globally optimized transmission plan, ensuring coordinated allocation of information. Across six question-answering benchmarks, ComprExIT consistently outperforms state-of-the-art context compression methods while introducing only ~1% additional parameters, demonstrating that explicit and coordinated information transmission enables more effective and robust long-context compression.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 14,
        "ai_keywords": [
          "soft context compression",
          "LLMs",
          "quadratic attention",
          "key-value caches",
          "layer-by-layer self-attention",
          "information transmission",
          "frozen LLM hidden states",
          "depth-wise transmission",
          "width-wise transmission",
          "token anchors",
          "compression capacity"
        ]
      }
    },
    {
      "id": "arxiv:2602.08658",
      "title": "Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models",
      "authors": [
        "Mingzi Cao",
        "Xingwei Tan",
        "Mahmud Akhter",
        "Marco Valentino",
        "Maria Liakata",
        "Xi Wang",
        "Nikolaos Aletras"
      ],
      "url": "https://huggingface.co/papers/2602.08658",
      "abstract": "Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to 14.60) across realistic tasks.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 13,
        "github_repo": "https://github.com/voalmciaf/FR-OOD",
        "ai_keywords": [
          "Large Language Model",
          "reasoning paradigms",
          "fine-tuning",
          "mixture-of-experts",
          "out-of-domain tasks",
          "generalizability"
        ]
      }
    },
    {
      "id": "arxiv:2602.06454",
      "title": "RelayGen: Intra-Generation Model Switching for Efficient Reasoning",
      "authors": [
        "Jiwon Song",
        "Yoongon Kim",
        "Jae-Joon Kim"
      ],
      "url": "https://huggingface.co/papers/2602.06454",
      "abstract": "Large reasoning models (LRMs) achieve strong performance on complex reasoning tasks by generating long, multi-step reasoning trajectories, but inference-time scaling incurs substantial deployment cost. A key challenge is that generation difficulty varies within a single output, whereas existing efficiency-oriented approaches either ignore this intra-generation variation or rely on supervised token-level routing with high system complexity. We present RelayGen, a training-free, segment-level runtime model switching framework that exploits difficulty variation in long-form reasoning. Through offline analysis of generation uncertainty using token probability margins, we show that coarse-grained segment-level control is sufficient to capture difficulty transitions within a reasoning trajectory. RelayGen identifies model-specific switch cues that signal transitions to lower-difficulty segments and dynamically delegates their continuation to a smaller model, while preserving high-difficulty reasoning on the large model. Across multiple reasoning benchmarks, RelayGen substantially reduces inference latency while preserving most of the accuracy of large models. When combined with speculative decoding, RelayGen achieves up to 2.2times end-to-end speedup with less than 2\\% accuracy degradation, without requiring additional training or learned routing components.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 11,
        "github_repo": "https://github.com/jiwonsong-dev/RelayGen",
        "ai_keywords": [
          "large reasoning models",
          "multi-step reasoning trajectories",
          "inference-time scaling",
          "token probability margins",
          "segment-level control",
          "model switching",
          "speculative decoding"
        ]
      }
    },
    {
      "id": "arxiv:2602.08236",
      "title": "When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning",
      "authors": [
        "Shoubin Yu",
        "Yue Zhang",
        "Zun Wang",
        "Jaehong Yoon",
        "Huaxiu Yao",
        "Mingyu Ding",
        "Mohit Bansal"
      ],
      "url": "https://huggingface.co/papers/2602.08236",
      "abstract": "Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 9,
        "github_repo": "https://github.com/Yui010206/Adaptive-Visual-Imagination-Control",
        "ai_keywords": [
          "Multimodal Large Language Models",
          "visual spatial reasoning",
          "world models",
          "visual imagination",
          "test-time adaptation",
          "embodied navigation",
          "SAT",
          "MMSI",
          "R2R"
        ]
      }
    },
    {
      "id": "arxiv:2602.08808",
      "title": "How2Everything: Mining the Web for How-To Procedures to Evaluate and Improve LLMs",
      "authors": [
        "Yapei Chang",
        "Kyle Lo",
        "Mohit Iyyer",
        "Luca Soldaini"
      ],
      "url": "https://huggingface.co/papers/2602.08808",
      "abstract": "Generating step-by-step \"how-to\" procedures is a key LLM capability: how-to advice is commonly requested in chatbots, and step-by-step planning is critical for reasoning over complex tasks. Yet, measuring and improving procedural validity at scale on real-world tasks remains challenging and understudied. To address this, we introduce How2Everything, a scalable framework to evaluate and improve goal-conditioned procedure generation. Our framework includes How2Mine, which mines 351K procedures from 980K web pages across 14 topics and readily scales to larger corpora. From this pool we build How2Bench, a 7K-example evaluation set balanced across topics. To reliably score model outputs, we develop How2Score, an evaluation protocol that uses an LLM judge to detect whether a generation contains any critical failure that would prevent achieving the goal. For low-cost, reproducible evaluation, we distill a frontier model into an open 8B model, achieving 80.5% agreement with human annotators. How2Bench reveals clear scaling trends across model sizes and training stages, providing signal early in pretraining. Finally, RL using How2Score as a reward improves performance on How2Bench by >10 points across three models without systematic regressions on standard benchmarks, with gains robust to superficial source-document memorization or format compliance. Taken together, How2Everything shows how pretraining web data can support a closed loop of capability evaluation and improvement at scale.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 8,
        "github_repo": "https://github.com/lilakk/how2everything",
        "ai_keywords": [
          "goal-conditioned procedure generation",
          "How2Mine",
          "How2Bench",
          "How2Score",
          "LLM judge",
          "distillation",
          "reinforcement learning",
          "pretraining",
          "closed loop evaluation"
        ]
      }
    },
    {
      "id": "arxiv:2602.08145",
      "title": "Reliable and Responsible Foundation Models: A Comprehensive Survey",
      "authors": [
        "Xinyu Yang",
        "Junlin Han",
        "Rishi Bommasani",
        "Jinqi Luo",
        "Wenjie Qu",
        "Wangchunshu Zhou",
        "Adel Bibi",
        "Xiyao Wang",
        "Jaehong Yoon",
        "Elias Stengel-Eskin",
        "Shengbang Tong",
        "Lingfeng Shen",
        "Rafael Rafailov",
        "Runjia Li",
        "Zhaoyang Wang",
        "Yiyang Zhou",
        "Chenhang Cui",
        "Yu Wang",
        "Wenhao Zheng",
        "Huichi Zhou",
        "Jindong Gu",
        "Zhaorun Chen",
        "Peng Xia",
        "Tony Lee",
        "Thomas Zollo",
        "Vikash Sehwag",
        "Jixuan Leng",
        "Jiuhai Chen",
        "Yuxin Wen",
        "Huan Zhang",
        "Zhun Deng",
        "Linjun Zhang",
        "Pavel Izmailov",
        "Pang Wei Koh",
        "Yulia Tsvetkov",
        "Andrew Wilson",
        "Jiaheng Zhang",
        "James Zou",
        "Cihang Xie",
        "Hao Wang",
        "Philip Torr",
        "Julian McAuley",
        "David Alvarez-Melis",
        "Florian Tram\u00e8r",
        "Kaidi Xu",
        "Suman Jana",
        "Chris Callison-Burch",
        "Rene Vidal",
        "Filippos Kokkinos",
        "Mohit Bansal",
        "Beidi Chen",
        "Huaxiu Yao"
      ],
      "url": "https://huggingface.co/papers/2602.08145",
      "abstract": "Foundation models, including Large Language Models (LLMs), Multimodal Large Language Models (MLLMs), Image Generative Models (i.e, Text-to-Image Models and Image-Editing Models), and Video Generative Models, have become essential tools with broad applications across various domains such as law, medicine, education, finance, science, and beyond. As these models see increasing real-world deployment, ensuring their reliability and responsibility has become critical for academia, industry, and government. This survey addresses the reliable and responsible development of foundation models. We explore critical issues, including bias and fairness, security and privacy, uncertainty, explainability, and distribution shift. Our research also covers model limitations, such as hallucinations, as well as methods like alignment and Artificial Intelligence-Generated Content (AIGC) detection. For each area, we review the current state of the field and outline concrete future research directions. Additionally, we discuss the intersections between these areas, highlighting their connections and shared challenges. We hope our survey fosters the development of foundation models that are not only powerful but also ethical, trustworthy, reliable, and socially responsible.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 8,
        "ai_keywords": [
          "Large Language Models",
          "Multimodal Large Language Models",
          "Text-to-Image Models",
          "Image-Editing Models",
          "Video Generative Models",
          "Artificial Intelligence-Generated Content",
          "hallucinations",
          "alignment",
          "AIGC detection",
          "bias",
          "fairness",
          "security",
          "privacy",
          "uncertainty",
          "explainability",
          "distribution shift",
          "model limitations"
        ]
      }
    },
    {
      "id": "arxiv:2602.07796",
      "title": "Thinking Makes LLM Agents Introverted: How Mandatory Thinking Can Backfire in User-Engaged Agents",
      "authors": [
        "Jiatong Li",
        "Changdae Oh",
        "Hyeong Kyu Choi",
        "Jindong Wang",
        "Sharon Li"
      ],
      "url": "https://huggingface.co/papers/2602.07796",
      "abstract": "Eliciting reasoning has emerged as a powerful technique for improving the performance of large language models (LLMs) on complex tasks by inducing thinking. However, their effectiveness in realistic user-engaged agent scenarios remains unclear. In this paper, we conduct a comprehensive study on the effect of explicit thinking in user-engaged LLM agents. Our experiments span across seven models, three benchmarks, and two thinking instantiations, and we evaluate them through both a quantitative response taxonomy analysis and qualitative failure propagation case studies. Contrary to expectations, we find that mandatory thinking often backfires on agents in user-engaged settings, causing anomalous performance degradation across various LLMs. Our key finding reveals that thinking makes agents more ``introverted'' by shortening responses and reducing information disclosure to users, which weakens agent-user information exchange and leads to downstream task failures. Furthermore, we demonstrate that explicitly prompting for information disclosure reliably improves performance across diverse model families, suggesting that proactive transparency is a vital lever for agent optimization. Overall, our study suggests that information transparency awareness is a crucial yet underexplored perspective for the future design of reasoning agents in real-world scenarios. Our code is available at https://github.com/deeplearning-wisc/Thinking-Agent.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 7,
        "ai_keywords": [
          "large language models",
          "reasoning",
          "agent scenarios",
          "information disclosure",
          "performance degradation",
          "transparent prompting"
        ]
      }
    },
    {
      "id": "arxiv:2602.07775",
      "title": "Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion",
      "authors": [
        "Haodong Li",
        "Shaoteng Liu",
        "Zhe Lin",
        "Manmohan Chandraker"
      ],
      "url": "https://huggingface.co/papers/2602.07775",
      "abstract": "Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 7,
        "ai_keywords": [
          "autoregressive video diffusion models",
          "train-test gap",
          "self forcing",
          "rolling sink",
          "AR cache maintenance",
          "long-horizon video synthesis",
          "temporal consistency",
          "visual fidelity"
        ]
      }
    },
    {
      "id": "arxiv:2602.07080",
      "title": "CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs",
      "authors": [
        "Yicheng He",
        "Zheng Zhao",
        "Zhou Kaiyu",
        "Bryan Dai",
        "Jie Fu",
        "Yonghui Yang"
      ],
      "url": "https://huggingface.co/papers/2602.07080",
      "abstract": "Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model's own capabilities. This raises a fundamental, yet unexplored question: Can an LLM's functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the model's neural dynamics encode internally decodable signals that are predictive of logical validity during code generation. Inspired by mechanistic interpretability, we propose to treat code verification as a mechanistic diagnostic task, mapping the model's explicit algorithmic trajectory into line-level attribution graphs. By decomposing complex residual flows, we aim to identify the structural signatures that distinguish sound reasoning from logical failure within the model's internal circuits. Analysis across Python, C++, and Java confirms that intrinsic correctness signals are robust across diverse syntaxes. Topological features from these internal graphs predict correctness more reliably than surface heuristics and enable targeted causal interventions to fix erroneous logic. These findings establish internal introspection as a decodable property for verifying generated code. Our code is at https:// github.com/bruno686/CodeCircuit.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 6,
        "github_repo": "https://github.com/bruno686/CodeCircuit",
        "ai_keywords": [
          "mechanistic interpretability",
          "code verification",
          "neural dynamics",
          "algorithmic trajectory",
          "line-level attribution graphs",
          "residual flows",
          "structural signatures",
          "logical validity",
          "computational circuits",
          "topological features",
          "causal interventions"
        ]
      }
    },
    {
      "id": "arxiv:2602.09003",
      "title": "Data Science and Technology Towards AGI Part I: Tiered Data Management",
      "authors": [
        "Yudong Wang",
        "Zixuan Fu",
        "Hengyu Zhao",
        "Chen Zhao",
        "Chuyue Zhou",
        "Xinle Lin",
        "Hongya Lyu",
        "Shuaikang Xue",
        "Yi Yi",
        "Yingjiao Wang",
        "Zhi Zheng",
        "Yuzhou Zhang",
        "Jie Zhou",
        "Chaojun Xiao",
        "Xu Han",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "url": "https://huggingface.co/papers/2602.09003",
      "abstract": "The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 5,
        "ai_keywords": [
          "large language models",
          "data management",
          "data quality",
          "training efficiency",
          "tiered data management",
          "data curation",
          "model-guided data refinement",
          "pre-training",
          "mid-training",
          "alignment"
        ]
      }
    },
    {
      "id": "arxiv:2602.07803",
      "title": "SoulX-Singer: Towards High-Quality Zero-Shot Singing Voice Synthesis",
      "authors": [
        "Jiale Qian",
        "Hao Meng",
        "Tian Zheng",
        "Pengcheng Zhu",
        "Haopeng Lin",
        "Yuhang Dai",
        "Hanke Xie",
        "Wenxiao Cao",
        "Ruixuan Shang",
        "Jun Wu",
        "Hongmei Liu",
        "Hanlin Wen",
        "Jian Zhao",
        "Zhonglin Jiang",
        "Yong Chen",
        "Shunshun Yin",
        "Ming Tao",
        "Jianguo Wei",
        "Lei Xie",
        "Xinsheng Wang"
      ],
      "url": "https://huggingface.co/papers/2602.07803",
      "abstract": "While recent years have witnessed rapid progress in speech synthesis, open-source singing voice synthesis (SVS) systems still face significant barriers to industrial deployment, particularly in terms of robustness and zero-shot generalization. In this report, we introduce SoulX-Singer, a high-quality open-source SVS system designed with practical deployment considerations in mind. SoulX-Singer supports controllable singing generation conditioned on either symbolic musical scores (MIDI) or melodic representations, enabling flexible and expressive control in real-world production workflows. Trained on more than 42,000 hours of vocal data, the system supports Mandarin Chinese, English, and Cantonese and consistently achieves state-of-the-art synthesis quality across languages under diverse musical conditions. Furthermore, to enable reliable evaluation of zero-shot SVS performance in practical scenarios, we construct SoulX-Singer-Eval, a dedicated benchmark with strict training-test disentanglement, facilitating systematic assessment in zero-shot settings.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 4,
        "ai_keywords": [
          "singing voice synthesis",
          "symbolic musical scores",
          "melodic representations",
          "zero-shot generalization",
          "speech synthesis",
          "Mandarin Chinese",
          "English",
          "Cantonese"
        ]
      }
    },
    {
      "id": "arxiv:2601.21363",
      "title": "Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control",
      "authors": [
        "Weidong Huang",
        "Zhehan Li",
        "Hangxin Liu",
        "Biao Hou",
        "Yao Su",
        "Jingwen Zhang"
      ],
      "url": "https://huggingface.co/papers/2601.21363",
      "abstract": "Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 4,
        "github_repo": "https://github.com/bigai-ai/LIFT-humanoid",
        "ai_keywords": [
          "Proximal Policy Optimization",
          "Soft Actor-Critic",
          "on-policy methods",
          "off-policy RL",
          "model-based RL",
          "large-scale parallel simulation",
          "zero-shot deployment",
          "sample efficiency",
          "large-batch update",
          "Update-To-Data ratio",
          "deterministic policy",
          "stochastic exploration",
          "physics-informed world model"
        ]
      }
    },
    {
      "id": "arxiv:2602.09782",
      "title": "Flexible Entropy Control in RLVR with Gradient-Preserving Perspective",
      "authors": [
        "Kun Chen",
        "Peng Shi",
        "Fanfan Liu",
        "Haibo Qiu",
        "Zhixiong Zeng",
        "Siqi Yang",
        "Wenji Mao"
      ],
      "url": "https://huggingface.co/papers/2602.09782",
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a critical method for enhancing the reasoning capabilities of Large Language Models (LLMs). However, continuous training often leads to policy entropy collapse, characterized by a rapid decay in entropy that results in premature overconfidence, reduced output diversity, and vanishing gradient norms that inhibit learning. Gradient-Preserving Clipping is a primary factor influencing these dynamics, but existing mitigation strategies are largely static and lack a framework connecting clipping mechanisms to precise entropy control. This paper proposes reshaping entropy control in RL from the perspective of Gradient-Preserving Clipping. We first theoretically and empirically verify the contributions of specific importance sampling ratio regions to entropy growth and reduction. Leveraging these findings, we introduce a novel regulation mechanism using dynamic clipping threshold to precisely manage entropy. Furthermore, we design and evaluate dynamic entropy control strategies, including increase-then-decrease, decrease-increase-decrease, and oscillatory decay. Experimental results demonstrate that these strategies effectively mitigate entropy collapse, and achieve superior performance across multiple benchmarks.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 3,
        "github_repo": "https://github.com/Kwen-Chen/Flexible-Entropy-Control",
        "ai_keywords": [
          "reinforcement learning",
          "large language models",
          "policy entropy collapse",
          "gradient-preserving clipping",
          "entropy control",
          "dynamic clipping threshold",
          "importance sampling ratio",
          "entropy growth",
          "entropy reduction",
          "dynamic entropy control strategies"
        ]
      }
    },
    {
      "id": "arxiv:2602.08961",
      "title": "MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE",
      "authors": [
        "Ruijie Zhu",
        "Jiahao Lu",
        "Wenbo Hu",
        "Xiaoguang Han",
        "Jianfei Cai",
        "Ying Shan",
        "Chuanxia Zheng"
      ],
      "url": "https://huggingface.co/papers/2602.08961",
      "abstract": "We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 3,
        "github_repo": "https://github.com/TencentARC/MotionCrafter",
        "ai_keywords": [
          "video diffusion",
          "4D geometry",
          "dense motion estimation",
          "3D point maps",
          "3D scene flows",
          "shared coordinate system",
          "4D VAE",
          "RGB VAE latents",
          "data normalization",
          "VAE training strategy",
          "diffusion priors"
        ]
      }
    },
    {
      "id": "arxiv:2602.08829",
      "title": "WildReward: Learning Reward Models from In-the-Wild Human Interactions",
      "authors": [
        "Hao Peng",
        "Yunjia Qi",
        "Xiaozhi Wang",
        "Zijun Yao",
        "Lei Hou",
        "Juanzi Li"
      ],
      "url": "https://huggingface.co/papers/2602.08829",
      "abstract": "Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 3,
        "ai_keywords": [
          "reward models",
          "large language models",
          "ordinal regression",
          "in-the-wild interactions",
          "user feedback",
          "online DPO training"
        ]
      }
    },
    {
      "id": "arxiv:2602.08004",
      "title": "Agent Skills: A Data-Driven Analysis of Claude Skills for Extending Large Language Model Functionality",
      "authors": [
        "George Ling",
        "Shanshan Zhong",
        "Richard Huang"
      ],
      "url": "https://huggingface.co/papers/2602.08004",
      "abstract": "Agent skills extend large language model (LLM) agents with reusable, program-like modules that define triggering conditions, procedural logic, and tool interactions. As these skills proliferate in public marketplaces, it is unclear what types are available, how users adopt them, and what risks they pose. To answer these questions, we conduct a large-scale, data-driven analysis of 40,285 publicly listed skills from a major marketplace. Our results show that skill publication tends to occur in short bursts that track shifts in community attention. We also find that skill content is highly concentrated in software engineering workflows, while information retrieval and content creation account for a substantial share of adoption. Beyond content trends, we uncover a pronounced supply-demand imbalance across categories, and we show that most skills remain within typical prompt budgets despite a heavy-tailed length distribution. Finally, we observe strong ecosystem homogeneity, with widespread intent-level redundancy, and we identify non-trivial safety risks, including skills that enable state-changing or system-level actions. Overall, our findings provide a quantitative snapshot of agent skills as an emerging infrastructure layer for agents and inform future work on skill reuse, standardization, and safety-aware design.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 3
      }
    },
    {
      "id": "arxiv:2602.06942",
      "title": "Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay",
      "authors": [
        "Duygu Altinok"
      ],
      "url": "https://huggingface.co/papers/2602.06942",
      "abstract": "Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer's training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a \"subwords manifest\", that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this \"subwords manifest\" delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 3,
        "ai_keywords": [
          "tokenization",
          "morphologically rich languages",
          "subword tokenization",
          "WordPiece",
          "morphology level",
          "character-level tokenization",
          "intrinsic diagnostics",
          "extrinsic evaluation",
          "NLI",
          "STS",
          "sentiment analysis",
          "NER",
          "POS",
          "dependency parsing",
          "morphology-aware diagnostic toolkit",
          "F1 score",
          "lemma atomicity",
          "segmentation indices",
          "CER",
          "WER",
          "continuation rates",
          "affix-type coverage",
          "token-level atomicity"
        ]
      }
    },
    {
      "id": "arxiv:2602.06445",
      "title": "ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking",
      "authors": [
        "Weidong Huang",
        "Jingwen Zhang",
        "Jiongye Li",
        "Shibowen Zhang",
        "Jiayang Wu",
        "Jiayi Wang",
        "Hangxin Liu",
        "Yaodong Yang",
        "Yao Su"
      ],
      "url": "https://huggingface.co/papers/2602.06445",
      "abstract": "Achieving stable and energy-efficient locomotion is essential for humanoid robots to operate continuously in real-world applications. Existing MPC and RL approaches often rely on energy-related metrics embedded within a multi-objective optimization framework, which require extensive hyperparameter tuning and often result in suboptimal policies. To address these challenges, we propose ECO (Energy-Constrained Optimization), a constrained RL framework that separates energy-related metrics from rewards, reformulating them as explicit inequality constraints. This method provides a clear and interpretable physical representation of energy costs, enabling more efficient and intuitive hyperparameter tuning for improved energy efficiency. ECO introduces dedicated constraints for energy consumption and reference motion, enforced by the Lagrangian method, to achieve stable, symmetric, and energy-efficient walking for humanoid robots. We evaluated ECO against MPC, standard RL with reward shaping, and four state-of-the-art constrained RL methods. Experiments, including sim-to-sim and sim-to-real transfers on the kid-sized humanoid robot BRUCE, demonstrate that ECO significantly reduces energy consumption compared to baselines while maintaining robust walking performance. These results highlight a substantial advancement in energy-efficient humanoid locomotion. All experimental demonstrations can be found on the project website: https://sites.google.com/view/eco-humanoid.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 3,
        "github_repo": "https://github.com/bigai-ai/ECO-humanoid",
        "ai_keywords": [
          "model predictive control",
          "reinforcement learning",
          "constrained optimization",
          "Lagrangian method",
          "energy-constrained optimization",
          "humanoid robotics",
          "sim-to-sim transfer",
          "sim-to-real transfer"
        ]
      }
    },
    {
      "id": "arxiv:2602.08818",
      "title": "FlexMoRE: A Flexible Mixture of Rank-heterogeneous Experts for Efficient Federatedly-trained Large Language Models",
      "authors": [
        "Annemette Brok Pirchert",
        "Jacob Nielsen",
        "Mogens Henrik From",
        "Lukas Galke Poech",
        "Peter Schneider-Kamp"
      ],
      "url": "https://huggingface.co/papers/2602.08818",
      "abstract": "Recent advances in mixture-of-experts architectures have shown that individual experts models can be trained federatedly, i.e., in isolation from other experts by using a common base model to facilitate coordination. However, we hypothesize that full-sized experts may not be necessary for all domains and that instead low-rank adapters may be sufficient. Here, we introduce FlexMoRE, a Flexible Mixture of Rank-heterogenous Experts, which may be either full-sized experts or adapters of a suitable rank. We systematically investigate the trade-off between expert rank and downstream task performance by evaluating 6 experts with ranks 2^0 to 2^{14} resulting in experiments covering 150 mixtures (96 with 2 experts, 54 with 7 experts) that are evaluated across 120 tasks. For our experiments, we build on FlexOlmo and turn its pre-trained experts into low-rank versions. Our regression analysis from expert rank to downstream task performance reveals that the best-performing rank is substantially higher for reasoning-heavy benchmarks than for knowledge-heavy benchmarks. These findings on rank sensitivity come with direct implications for memory efficiency: Using optimal ranks, FlexMoRE yields improved downstream task performance (average score 47.18) compared to the baseline FlexOlmo-style mixture of full-sized experts (average score 45.46) at less than one third the parameters (10.75B for FlexMoRE vs. 33.27B for FlexOlmo). All code will be made available.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 2,
        "ai_keywords": [
          "mixture-of-experts architectures",
          "low-rank adapters",
          "rank-heterogenous experts",
          "downstream task performance",
          "regression analysis",
          "parameter efficiency"
        ]
      }
    },
    {
      "id": "arxiv:2602.07150",
      "title": "On Randomness in Agentic Evals",
      "authors": [
        "Bjarni Haukur Bjarnason",
        "Andr\u00e9 Silva",
        "Martin Monperrus"
      ],
      "url": "https://huggingface.co/papers/2602.07150",
      "abstract": "Agentic systems are evaluated on benchmarks where agents interact with environments to solve tasks. Most papers report a pass@1 score computed from a single run per task, assuming this gives a reliable performance estimate. We test this assumption by collecting 60,000 agentic trajectories on SWE-Bench-Verified, spanning three models and two scaffolds. We find substantial variance: single-run pass@1 estimates vary by 2.2 to 6.0 percentage points depending on which run is selected, with standard deviations exceeding 1.5 percentage points even at temperature 0. This variance has critical implications: reported improvements of 2--3 percentage points may reflect evaluation noise rather than genuine algorithmic progress. Through token-level analysis, we show that trajectories diverge early, often within the first few percent of tokens, and that these small differences cascade into different solution strategies. To enable reliable evaluation of agentic systems, we recommend three concrete practices: (1) estimate pass@1 from multiple independent runs per task, especially when measuring small improvements, (2) use statistical power analysis to determine the number of runs needed to detect expected effect sizes, and (3) consider metrics like pass@k (optimistic bound) and pass^k (pessimistic bound) with k>1 to better characterize the full performance envelope. While these practices increase evaluation cost, they are essential for distinguishing genuine scientific progress from statistical noise.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 2,
        "ai_keywords": [
          "pass@1",
          "pass@k",
          "pass^k",
          "SWE-Bench-Verified",
          "agentic trajectories",
          "statistical power analysis",
          "evaluation variance",
          "solution strategies"
        ]
      }
    },
    {
      "id": "arxiv:2602.06600",
      "title": "Echoes as Anchors: Probabilistic Costs and Attention Refocusing in LLM Reasoning",
      "authors": [
        "Zhuoyuan Hao",
        "Zhuo Li",
        "Wu Li",
        "Fangming Liu",
        "Min Zhang",
        "Jing Li"
      ],
      "url": "https://huggingface.co/papers/2602.06600",
      "abstract": "Test-time compute allocation in large reasoning models (LRMs) is widely used and has applications in mathematical problem solving, code synthesis, and planning. Recent work has addressed this problem by scaling self-consistency and parallel thinking, adding generic ``thinking tokens'' and prompting models to re-read the question before answering. Unfortunately, these approaches either inject task-agnostic tokens or mandate heuristics that do not explain -- and often ignore -- the spontaneous repetition that many LRMs exhibit at the head of their internal chains. In contrast, we analyze and harness the model's tendency to restate the question, which we term the Echo of Prompt (EOP), as a front-loaded, compute-shaping mechanism. We formalize its probabilistic cost by casting echo removal as rejection-based conditioning and defining the Echo Likelihood Gap \u0394L as a computable proxy. This provides the missing theoretical link that links early repetition to likelihood gains and downstream accuracy. However, it does not by itself specify how to exploit EOP. Consequently, we develop Echo-Distilled SFT (ED-SFT) to instill an ``echo-then-reason'' pattern through supervised finetuning, and Echoic Prompting (EP) to re-ground the model mid-trace without training. While promising, quantifying benefits beyond verbosity is non-trivial. Therefore, we conduct length and suffix-controlled likelihood analyses together with layer-wise attention studies, showing that EOP increases answer to answer-prefix attention in middle layers, consistent with an attention refocusing mechanism. We evaluate on GSM8K, MathQA, Hendrycks-MATH, AIME24, and MATH-500 under identical decoding settings and budgets, and find consistent gains over baselines. Code is available at https://github.com/hhh2210/echoes-as-anchors.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 2,
        "github_repo": "https://github.com/hhh2210/echoes-as-anchors",
        "ai_keywords": [
          "large reasoning models",
          "self-consistency",
          "parallel thinking",
          "thinking tokens",
          "echo of prompt",
          "echo removal",
          "rejection-based conditioning",
          "echo likelihood gap",
          "echo-distilled sft",
          "echoic prompting",
          "attention refocusing",
          "likelihood analysis",
          "layer-wise attention"
        ]
      }
    },
    {
      "id": "arxiv:2602.05929",
      "title": "KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs",
      "authors": [
        "Jian Chen",
        "Zhuoran Wang",
        "Jiayu Qin",
        "Ming Li",
        "Meng Wang",
        "Changyou Chen",
        "Yin Chen",
        "Qizhen Weng",
        "Yirui Liu"
      ],
      "url": "https://huggingface.co/papers/2602.05929",
      "abstract": "Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 2,
        "ai_keywords": [
          "KV-cache",
          "SVD-based method",
          "low-rank approximation",
          "Frobenius norm",
          "gradient-free",
          "incremental evaluation",
          "Normalized Effective Rank",
          "compressibility",
          "autoregressive decoding",
          "GPU memory bandwidth",
          "KV-cache compression",
          "dataset-level evaluation",
          "layer-wise evaluation"
        ]
      }
    },
    {
      "id": "arxiv:2602.07040",
      "title": "Aster: Autonomous Scientific Discovery over 20x Faster Than Existing Methods",
      "authors": [
        "Emmett Bicker"
      ],
      "url": "https://huggingface.co/papers/2602.07040",
      "abstract": "We introduce Aster, an AI agent for autonomous scientific discovery capable of operating over 20 times faster than existing frameworks. Given a task, an initial program, and a script to evaluate the performance of the program, Aster iteratively improves the program, often leading to new state-of-the-art performances. Aster's significant reduction in the number of iterations required for novel discovery expands the domain of tractable problems to include tasks with long evaluation durations, such as multi-hour machine learning training runs.\n  We applied Aster to problems in mathematics, GPU kernel engineering, biology, neuroscience, and language model training. More specifically: the Erdos minimum overlap problem, optimizing the TriMul kernel, a single-cell analysis denoising problem, training a neural activity prediction model to perform well on ZAPBench, and the NanoGPT Speedrun Competition. Aster attains SOTA results in every task, except for ZAPBench, where it matches the performance of the best human solution with less than 1/190th of the compute.\n  Aster is accessible via a web interface and API at asterlab.ai.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 2,
        "ai_keywords": [
          "AI agent",
          "autonomous scientific discovery",
          "iterative program improvement",
          "state-of-the-art performance",
          "computational efficiency",
          "web interface",
          "API"
        ]
      }
    },
    {
      "id": "arxiv:2602.02827",
      "title": "Col-Bandit: Zero-Shot Query-Time Pruning for Late-Interaction Retrieval",
      "authors": [
        "Roi Pony",
        "Adi Raz",
        "Oshri Naparstek",
        "Idan Friedman",
        "Udi Barzelay"
      ],
      "url": "https://huggingface.co/papers/2602.02827",
      "abstract": "Multi-vector late-interaction retrievers such as ColBERT achieve state-of-the-art retrieval quality, but their query-time cost is dominated by exhaustively computing token-level MaxSim interactions for every candidate document. While approximating late interaction with single-vector representations reduces cost, it often incurs substantial accuracy loss. We introduce Col-Bandit, a query-time pruning algorithm that reduces this computational burden by casting reranking as a finite-population Top-K identification problem. Col-Bandit maintains uncertainty-aware bounds over partially observed document scores and adaptively reveals only the (document, query token) MaxSim entries needed to determine the top results under statistical decision bounds with a tunable relaxation. Unlike coarse-grained approaches that prune entire documents or tokens offline, Col-Bandit sparsifies the interaction matrix on the fly. It operates as a zero-shot, drop-in layer over standard multi-vector systems, requiring no index modifications, offline preprocessing, or model retraining. Experiments on textual (BEIR) and multimodal (REAL-MM-RAG) benchmarks show that Col-Bandit preserves ranking fidelity while reducing MaxSim FLOPs by up to 5times, indicating that dense late-interaction scoring contains substantial redundancy that can be identified and pruned efficiently at query time.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 2,
        "ai_keywords": [
          "multi-vector late-interaction retrievers",
          "ColBERT",
          "MaxSim interactions",
          "reranking",
          "Top-K identification",
          "finite-population",
          "uncertainty-aware bounds",
          "query-time pruning",
          "sparse interaction matrix",
          "dense late-interaction scoring"
        ]
      }
    },
    {
      "id": "arxiv:2602.08629",
      "title": "CauScale: Neural Causal Discovery at Scale",
      "authors": [
        "Bo Peng",
        "Sirui Chen",
        "Jiaguo Tian",
        "Yu Qiao",
        "Chaochao Lu"
      ],
      "url": "https://huggingface.co/papers/2602.08629",
      "abstract": "Causal discovery is essential for advancing data-driven fields such as scientific AI and data analysis, yet existing approaches face significant time- and space-efficiency bottlenecks when scaling to large graphs. To address this challenge, we present CauScale, a neural architecture designed for efficient causal discovery that scales inference to graphs with up to 1000 nodes. CauScale improves time efficiency via a reduction unit that compresses data embeddings and improves space efficiency by adopting tied attention weights to avoid maintaining axis-specific attention maps. To keep high causal discovery accuracy, CauScale adopts a two-stream design: a data stream extracts relational evidence from high-dimensional observations, while a graph stream integrates statistical graph priors and preserves key structural signals. CauScale successfully scales to 500-node graphs during training, where prior work fails due to space limitations. Across testing data with varying graph scales and causal mechanisms, CauScale achieves 99.6% mAP on in-distribution data and 84.4% on out-of-distribution data, while delivering 4-13,000 times inference speedups over prior methods. Our project page is at https://github.com/OpenCausaLab/CauScale.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 1,
        "ai_keywords": [
          "causal discovery",
          "neural architecture",
          "data embeddings",
          "tied attention weights",
          "two-stream design",
          "relational evidence",
          "statistical graph priors",
          "structural signals",
          "mAP",
          "inference speedups"
        ]
      }
    },
    {
      "id": "arxiv:2602.07491",
      "title": "GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design",
      "authors": [
        "Isabella A. Stewart",
        "Tarjei Paule Hage",
        "Yu-Chuan Hsu",
        "Markus J. Buehler"
      ],
      "url": "https://huggingface.co/papers/2602.07491",
      "abstract": "Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science, where innovation demands integrating concepts from molecular chemistry to mechanical performance, this is especially acute. Neither humans nor single-agent LLMs can fully contend with this torrent of information, with the latter often prone to hallucinations. To address this bottleneck, we introduce a multi-agent framework guided by large-scale knowledge graphs to find sustainable substitutes for per- and polyfluoroalkyl substances (PFAS)-chemicals currently under intense regulatory scrutiny. Agents in the framework specialize in problem decomposition, evidence retrieval, design parameter extraction, and graph traversal, uncovering latent connections across distinct knowledge pockets to support hypothesis generation. Ablation studies show that the full multi-agent pipeline outperforms single-shot prompting, underscoring the value of distributed specialization and relational reasoning. We demonstrate that by tailoring graph traversal strategies, the system alternates between exploitative searches focusing on domain-critical outcomes and exploratory searches surfacing emergent cross-connections. Illustrated through the exemplar of biomedical tubing, the framework generates sustainable PFAS-free alternatives that balance tribological performance, thermal stability, chemical resistance, and biocompatibility. This work establishes a framework combining knowledge graphs with multi-agent reasoning to expand the materials design space, showcasing several initial design candidates to demonstrate the approach.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 1,
        "ai_keywords": [
          "large language models",
          "multi-agent framework",
          "knowledge graphs",
          "materials science",
          "problem decomposition",
          "evidence retrieval",
          "graph traversal",
          "hypothesis generation",
          "ablation studies",
          "domain-spanning reasoning",
          "sustainable substitutes",
          "PFAS chemicals",
          "tribological performance",
          "thermal stability",
          "chemical resistance",
          "biocompatibility"
        ]
      }
    },
    {
      "id": "arxiv:2602.05708",
      "title": "Cost-Efficient RAG for Entity Matching with LLMs: A Blocking-based Exploration",
      "authors": [
        "Chuangtao Ma",
        "Zeyu Zhang",
        "Arijit Khan",
        "Sebastian Schelter",
        "Paul Groth"
      ],
      "url": "https://huggingface.co/papers/2602.05708",
      "abstract": "Retrieval-augmented generation (RAG) enhances LLM reasoning in knowledge-intensive tasks, but existing RAG pipelines incur substantial retrieval and generation overhead when applied to large-scale entity matching. To address this limitation, we introduce CE-RAG4EM, a cost-efficient RAG architecture that reduces computation through blocking-based batch retrieval and generation. We also present a unified framework for analyzing and evaluating RAG systems for entity matching, focusing on blocking-aware optimizations and retrieval granularity. Extensive experiments suggest that CE-RAG4EM can achieve comparable or improved matching quality while substantially reducing end-to-end runtime relative to strong baselines. Our analysis further reveals that key configuration parameters introduce an inherent trade-off between performance and overhead, offering practical guidance for designing efficient and scalable RAG systems for entity matching and data integration.",
      "discovered_by": "hf_daily_papers",
      "discovered_date": "2026-02-10",
      "status": "discovered",
      "insights": [],
      "_meta": {
        "hf_upvotes": 1,
        "github_repo": "https://github.com/machuangtao/CE-RAG4EM",
        "ai_keywords": [
          "retrieval-augmented generation",
          "large-scale entity matching",
          "blocking-based batch retrieval",
          "generation overhead",
          "entity matching",
          "data integration"
        ]
      }
    },
    {
      "id": "arxiv:2602.06570",
      "title": "Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making",
      "authors": [
        "Baichuan-M3 Team",
        " :",
        "Chengfeng Dou",
        "Fan Yang",
        "Fei Li",
        "Jiyuan Jia",
        "Qiang Ju",
        "Shuai Wang",
        "Tianpeng Li",
        "Xiangrong Zeng",
        "Yijie Zhou",
        "Hongda Zhang",
        "Jinyang Tai",
        "Linzhuang Sun",
        "Peidong Guo",
        "Yichuan Mo",
        "Xiaochuan Wang",
        "Hengfu Cui",
        "Zhishou Zhang"
      ],
      "url": "https://arxiv.org/abs/2602.06570",
      "abstract": "We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at https://huggingface.co/collections/baichuan-inc/baichuan-m3.",
      "discovered_by": "arxiv_api",
      "discovered_date": "2026-02-06",
      "status": "analyzed",
      "insights": [
        "Shifts medical AI from passive question-answering to active clinical inquiry, mimicking how physicians systematically gather patient information",
        "Implements three core capabilities: proactive information gathering, long-horizon reasoning to connect scattered evidence, and adaptive hallucination suppression for factual accuracy",
        "Achieves state-of-the-art performance on HealthBench, HealthBench-Hallu, and ScanBench, outperforming GPT-5.2 in clinical inquiry, advisory quality, and safety",
        "Uses specialized training pipeline to model systematic physician workflows for more reliable medical decision support"
      ]
    },
    {
      "id": "arxiv:2212.08073",
      "title": "Constitutional AI: Harmlessness from AI Feedback",
      "authors": [
        "Yuntao Bai",
        "Saurav Kadavath",
        "Sandipan Kundu",
        "Amanda Askell",
        "Jackson Kernion",
        "Andy Jones",
        "Anna Chen",
        "Anna Goldie",
        "Azalia Mirhoseini",
        "Cameron McKinnon",
        "Carol Chen",
        "Catherine Olsson",
        "Christopher Olah",
        "Danny Hernandez",
        "Dawn Drain",
        "Deep Ganguli",
        "Dustin Li",
        "Eli Tran-Johnson",
        "Ethan Perez",
        "Jamie Kerr",
        "Jared Mueller",
        "Jeffrey Ladish",
        "Joshua Landau",
        "Kamal Ndousse",
        "Kamile Lukosuite",
        "Liane Lovitt",
        "Michael Sellitto",
        "Nelson Elhage",
        "Nicholas Schiefer",
        "Noemi Mercado",
        "Nova DasSarma",
        "Robert Lasenby",
        "Robin Larson",
        "Sam Ringer",
        "Scott Johnston",
        "Shauna Kravec",
        "Sheer El Showk",
        "Stanislav Fort",
        "Tamera Lanham",
        "Timothy Telleen-Lawton",
        "Tom Conerly",
        "Tom Henighan",
        "Tristan Hume",
        "Samuel R. Bowman",
        "Zac Hatfield-Dodds",
        "Ben Mann",
        "Dario Amodei",
        "Nicholas Joseph",
        "Sam McCandlish",
        "Tom Brown",
        "Jared Kaplan"
      ],
      "url": "https://arxiv.org/abs/2212.08073",
      "abstract": "As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.",
      "discovered_by": "arxiv_api",
      "discovered_date": "2022-12-15",
      "status": "cited",
      "insights": [
        "AI systems can be trained to be harmless using only a list of principles rather than extensive human labeling of harmful outputs",
        "The method combines supervised learning (self-critique and revision) with reinforcement learning from AI feedback (RLAIF) instead of human feedback",
        "The approach produces AI assistants that engage with harmful queries by explaining objections rather than refusing to respond",
        "Chain-of-thought reasoning can be incorporated to improve transparency and performance of AI decision-making",
        "The technique significantly reduces the need for human oversight while maintaining control over AI behavior",
        "Constitutional AI validates GodelAI C-S-P framework with empirical evidence"
      ]
    },
    {
      "id": "arxiv:2303.17651",
      "title": "Self-Refine: Iterative Refinement with Self-Feedback",
      "authors": [
        "Aman Madaan",
        "Niket Tandon",
        "Prakhar Gupta",
        "Skyler Hallinan",
        "Luyu Gao",
        "Sarah Wiegreffe",
        "Uri Alon",
        "Nouha Dziri",
        "Shrimai Prabhumoye",
        "Yiming Yang",
        "Shashank Gupta",
        "Bodhisattwa Prasad Majumder",
        "Katherine Hermann",
        "Sean Welleck",
        "Amir Yazdanbakhsh",
        "Peter Clark"
      ],
      "url": "https://arxiv.org/abs/2303.17651",
      "abstract": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.",
      "discovered_by": "arxiv_api",
      "discovered_date": "2023-03-30",
      "status": "cited",
      "insights": [
        "Large language models can effectively critique and improve their own outputs without requiring additional training, supervised data, or reinforcement learning",
        "Iterative self-refinement achieves approximately 20% absolute improvement in task performance across diverse applications including dialogue, math reasoning, and text generation",
        "The same language model can successfully serve three roles simultaneously: initial generator, feedback provider, and output refiner",
        "Even state-of-the-art models like GPT-4 benefit from this test-time improvement approach, demonstrating that first-pass outputs are often suboptimal",
        "Self-Refine proves inference-time iterative improvement without retraining"
      ]
    },
    {
      "id": "arxiv:2210.03629",
      "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
      "authors": [
        "Shunyu Yao",
        "Jeffrey Zhao",
        "Dian Yu",
        "Nan Du",
        "Izhak Shafran",
        "Karthik Narasimhan",
        "Yuan Cao"
      ],
      "url": "https://arxiv.org/abs/2210.03629",
      "abstract": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",
      "discovered_by": "arxiv_api",
      "discovered_date": "2022-10-06",
      "status": "cited",
      "insights": [
        "Interleaving reasoning traces with task-specific actions creates synergy where reasoning guides action planning and actions provide external information to improve reasoning",
        "ReAct reduces hallucination and error propagation in chain-of-thought reasoning by grounding responses in external knowledge sources through API interactions",
        "The method achieves 34% and 10% absolute success rate improvements over imitation and reinforcement learning on interactive decision-making tasks with minimal in-context examples",
        "Generated trajectories are more interpretable and human-like compared to methods using only reasoning or only acting",
        "The approach works effectively across diverse tasks including question answering, fact verification, and interactive decision making with simple prompting",
        "Interleaving reasoning traces with task-specific actions creates synergy where reasoning helps plan and handle exceptions while actions gather external information",
        "ReAct reduces hallucination and error propagation in chain-of-thought reasoning by grounding responses through interaction with external knowledge sources like Wikipedia",
        "The method achieves substantial improvements over baselines: 34% higher success rate than imitation learning on ALFWorld and 10% higher on WebShop with minimal in-context examples",
        "ReAct generates more interpretable and human-like task-solving trajectories compared to methods using only reasoning or only acting",
        "ReAct provides theoretical foundation for GodelAI multi-agent architecture"
      ]
    }
  ]
}