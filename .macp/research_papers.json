{
  "papers": [
    {
      "id": "arxiv:2502.04128",
      "title": "Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis",
      "authors": [
        "Zhen Ye",
        "Xinfa Zhu",
        "Chi-Min Chan",
        "Xinsheng Wang",
        "Xu Tan",
        "Jiahe Lei",
        "Yi Peng",
        "Haohe Liu",
        "Yizhu Jin",
        "Zheqi Dai",
        "Hongzhan Lin",
        "Jianyi Chen",
        "Xingjian Du",
        "Liumeng Xue",
        "Yunlin Chen",
        "Zhifei Li",
        "Lei Xie",
        "Qiuqiang Kong",
        "Yike Guo",
        "Wei Xue"
      ],
      "url": "https://arxiv.org/abs/2502.04128",
      "abstract": "Recent advances in text-based large language models (LLMs), particularly in the GPT series and the o1 model, have demonstrated the effectiveness of scaling both training-time and inference-time compute. However, current state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring separate models (e.g., diffusion models after LLM), complicating the decision of whether to scale a particular model during training or testing. This work makes the following contributions: First, we explore the scaling of train-time and inference-time compute for speech synthesis. Second, we propose a simple framework Llasa for speech synthesis that employs a single-layer vector quantizer (VQ) codec and a single Transformer architecture to fully align with standard LLMs such as Llama. Our experiments reveal that scaling train-time compute for Llasa consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns. Furthermore, from the perspective of scaling inference-time compute, we employ speech understanding models as verifiers during the search, finding that scaling inference-time compute shifts the sampling modes toward the preferences of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy. In addition, we released the checkpoint and training code for our TTS model (1B, 3B, 8B) and codec model publicly available.",
      "discovered_by": "arxiv_api",
      "discovered_date": "2025-02-06",
      "status": "discovered",
      "insights": []
    },
    {
      "id": "arxiv:2501.04306",
      "title": "LLM4SR: A Survey on Large Language Models for Scientific Research",
      "authors": [
        "Ziming Luo",
        "Zonglin Yang",
        "Zexin Xu",
        "Wei Yang",
        "Xinya Du"
      ],
      "url": "https://arxiv.org/abs/2501.04306",
      "abstract": "In recent years, the rapid advancement of Large Language Models (LLMs) has transformed the landscape of scientific research, offering unprecedented support across various stages of the research cycle. This paper presents the first systematic survey dedicated to exploring how LLMs are revolutionizing the scientific research process. We analyze the unique roles LLMs play across four critical stages of research: hypothesis discovery, experiment planning and implementation, scientific writing, and peer reviewing. Our review comprehensively showcases the task-specific methodologies and evaluation benchmarks. By identifying current challenges and proposing future research directions, this survey not only highlights the transformative potential of LLMs, but also aims to inspire and guide researchers and practitioners in leveraging LLMs to advance scientific inquiry. Resources are available at the following repository: https://github.com/du-nlp-lab/LLM4SR",
      "discovered_by": "arxiv_api",
      "discovered_date": "2025-01-08",
      "status": "cited",
      "insights": [
        "LLMs are being applied across the entire scientific research lifecycle, from initial hypothesis generation through peer review, representing a fundamental shift in how research is conducted",
        "Each research stage requires task-specific methodologies and evaluation benchmarks tailored to the unique requirements of scientific work",
        "Current LLM applications in science face significant challenges including domain-specific knowledge limitations, evaluation difficulties, and the need for specialized benchmarks",
        "The integration of LLMs into scientific workflows has transformative potential but requires careful consideration of reliability, reproducibility, and domain expertise",
        "Each stage of research has developed task-specific methodologies and evaluation benchmarks for LLM integration, though standardization remains limited",
        "Current challenges include ensuring factual accuracy, handling domain-specific knowledge, maintaining scientific rigor, and addressing ethical concerns about AI-generated research",
        "The survey identifies significant gaps in evaluation frameworks and the need for better integration of LLMs with existing scientific tools and databases"
      ]
    },
    {
      "id": "arxiv:2501.12946",
      "title": "Less is More: Simple yet Effective Heuristic Community Detection with Graph Convolution Network",
      "authors": [
        "Hong Wang",
        "Yinglong Zhang",
        "Zhangqi Zhao",
        "Zhicong Cai",
        "Xuewen Xia",
        "Xing Xu"
      ],
      "url": "https://arxiv.org/abs/2501.12946",
      "abstract": "Community detection is crucial in data mining. Traditional methods primarily focus on graph structure, often neglecting the significance of attribute features. In contrast, deep learning-based approaches incorporate attribute features and local structural information through contrastive learning, improving detection performance. However, existing algorithms' complex design and joint optimization make them difficult to train and reduce detection efficiency. Additionally, these methods require the number of communities to be predefined, making the results susceptible to artificial interference. To address these challenges, we propose a simple yet effective community detection algorithm that can adaptively detect communities without relying on data augmentation and contrastive optimization. The proposed algorithm first performs community pre-detection to extract global structural information adaptively. It then utilizes GCN to integrate local structures and attribute features. Subsequently, it combines global, local structures and attribute features in the feature space to discover community affiliations. Finally, a modularity maximization method is employed to optimize the communities based on these three types of information, thereby uncovering the community affiliation of each node. We conduct experimental comparisons across various graph datasets, evaluating the proposed algorithm against traditional methods and state-of-the-art community detection algorithms. The experimental results demonstrate that our algorithm achieves greater efficiency and accuracy in terms of both detection speed and effectiveness. The code is available at https://github.com/wuanghoong/Less-is-More.git.",
      "discovered_by": "arxiv_api",
      "discovered_date": "2025-01-22",
      "status": "discovered",
      "insights": []
    },
    {
      "id": "arxiv:2502.11271",
      "title": "OctoTools: An Agentic Framework with Extensible Tools for Complex Reasoning",
      "authors": [
        "Pan Lu",
        "Bowen Chen",
        "Sheng Liu",
        "Rahul Thapa",
        "Joseph Boen",
        "James Zou"
      ],
      "url": "https://arxiv.org/abs/2502.11271",
      "abstract": "Solving complex reasoning tasks may involve visual understanding, domain knowledge retrieval, numerical calculation, and multi-step reasoning. Existing methods augment large language models (LLMs) with external tools but are restricted to specialized domains, limited tool types, or require additional training data. In this paper, we introduce OctoTools, a training-free, user-friendly, and easily extensible open-source agentic framework designed to tackle complex reasoning across diverse domains. OctoTools introduces standardized tool cards to encapsulate tool functionality, a planner for both high-level and low-level planning, and an executor to carry out tool usage. We validate OctoTools' generality across 16 diverse tasks (including MathVista, MMLU-Pro, MedQA, and GAIA-Text), achieving substantial average accuracy gains of 9.3% over GPT-4o. Furthermore, OctoTools outperforms AutoGen, GPT-Functions and LangChain by up to 10.6% when given the same set of tools. Through comprehensive analysis and ablations, OctoTools demonstrates advantages in task planning, effective tool usage, and multi-step problem solving.",
      "discovered_by": "arxiv_api",
      "discovered_date": "2025-02-16",
      "status": "cited",
      "insights": [
        "OctoTools achieves 9.3% average accuracy improvement over GPT-4o across 16 diverse reasoning tasks without requiring additional training",
        "The framework outperforms existing agentic systems (AutoGen, GPT-Functions, LangChain) by up to 10.6% when using the same tools",
        "Standardized tool cards enable easy extensibility and consistent tool integration across different domains",
        "The dual-level planning approach (high-level and low-level) effectively decomposes complex multi-step reasoning problems",
        "Training-free design allows immediate deployment and adaptation to new domains and tools",
        "The framework outperforms existing tool-augmented systems (AutoGen, GPT-Functions, LangChain) by up to 10.6% when using the same tools",
        "The dual-level planning approach (high-level and low-level) combined with an executor enables effective multi-step reasoning and tool orchestration",
        "The training-free nature makes it more practical and accessible compared to methods requiring specialized training data"
      ]
    },
    {
      "id": "arxiv:2511.00086",
      "title": "Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph",
      "authors": [
        "Fali Wang",
        "Jihai Chen",
        "Shuhua Yang",
        "Runxue Bao",
        "Tianxiang Zhao",
        "Zhiwei Zhang",
        "Xianfeng Tang",
        "Hui Liu",
        "Qi He",
        "Suhang Wang"
      ],
      "url": "https://huggingface.co/papers/2511.00086",
      "abstract": "Test-Time Scaling (TTS) improves large language models (LLMs) by allocating additional computation during inference, typically through parallel, sequential, or hybrid scaling. However, prior studies often assume fixed collaboration architectures (e.g., topologies) and single-model usage, overlooking that optimal architectures and model combinations can vary across tasks. Therefore, we study the novel problem of searching for compute-optimal model combinations and architectures in TTS under a fixed budget. We formalize it as a multi-LLM collaboration graph, where nodes encode roles and LLM model assignments, and edges capture information flow. This problem is challenging because (i) the combinatorial search space is prohibitively large, and (ii) task-specific requirements demand tailored designs. To address these, we reformulate the problem as probabilistic graph optimization and, through pilot experiments, derive three empirical insights into TTS collaboration graphs. Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented framework that mirrors the REINFORCE pipeline by mapping sampling-gradient-update to sampling-feedback-update, where feedback serves as a textual gradient to update the probabilistic graph and efficiently search for optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE outperforms both traditional and LLM-based baselines in sample efficiency and search performance, and effectively identifies optimal graphs under joint objectives of accuracy and inference latency.",
      "discovered_by": "hysts_dataset",
      "discovered_date": "2025-11-04",
      "status": "discovered",
      "insights": []
    },
    {
      "id": "arxiv:2510.00507",
      "title": "Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs",
      "authors": [
        "Yurun Chen",
        "Xavier Hu",
        "Yuhan Liu",
        "Ziqi Wang",
        "Zeyi Liao",
        "Lin Chen",
        "Feng Wei",
        "Yuxi Qian",
        "Bo Zheng",
        "Keting Yin",
        "Shengyu Zhang"
      ],
      "url": "https://huggingface.co/papers/2510.00507",
      "abstract": "As multimodal LLM-driven agents continue to advance in autonomy and generalization, evaluation based on static datasets can no longer adequately assess their true capabilities in dynamic environments and diverse tasks. Existing LLM-based synthetic data methods are largely designed for LLM training and evaluation, and thus cannot be directly applied to agent tasks that require tool use and interactive capabilities. While recent studies have explored automatic agent task generation with LLMs, most efforts remain limited to text or image analysis, without systematically modeling multi-step interactions in web environments. To address these challenges, we propose Graph2Eval, a knowledge graph-based framework that automatically generates both multimodal document comprehension tasks and web interaction tasks, enabling comprehensive evaluation of agents' reasoning, collaboration, and interactive capabilities. In our approach, knowledge graphs constructed from multi-source external data serve as the task space, where we translate semantic relations into structured multimodal tasks using subgraph sampling, task templates, and meta-paths. A multi-stage filtering pipeline based on node reachability, LLM scoring, and similarity analysis is applied to guarantee the quality and executability of the generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of multiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures reasoning, collaboration, and interaction capabilities. We instantiate the framework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning document comprehension and web interaction scenarios. Experiments show that Graph2Eval efficiently generates tasks that differentiate agent and model performance, revealing gaps in reasoning, collaboration, and web interaction across different settings and offering a new perspective for agent evaluation.",
      "discovered_by": "hysts_dataset",
      "discovered_date": "2025-10-07",
      "status": "discovered",
      "insights": []
    },
    {
      "id": "arxiv:2309.09971",
      "title": "MindAgent: Emergent Gaming Interaction",
      "authors": [
        "Ran Gong",
        "Qiuyuan Huang",
        "Xiaojian Ma",
        "Hoi Vo",
        "Zane Durante",
        "Yusuke Noda",
        "Zilong Zheng",
        "Song-Chun Zhu",
        "Demetri Terzopoulos",
        "Li Fei-Fei",
        "Jianfeng Gao"
      ],
      "url": "https://huggingface.co/papers/2309.09971",
      "abstract": "Large Language Models (LLMs) have the capacity of performing complex scheduling in a multi-agent system and can coordinate these agents into completing sophisticated tasks that require extensive collaboration. However, despite the introduction of numerous gaming frameworks, the community has insufficient benchmarks towards building general multi-agents collaboration infrastructure that encompass both LLM and human-NPCs collaborations. In this work, we propose a novel infrastructure - MindAgent - to evaluate planning and coordination emergent capabilities for gaming interaction. In particular, our infrastructure leverages existing gaming framework, to i) require understanding of the coordinator for a multi-agent system, ii) collaborate with human players via un-finetuned proper instructions, and iii) establish an in-context learning on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new gaming scenario and related benchmark that dispatch a multi-agent collaboration efficiency and supervise multiple agents playing the game simultaneously. We conduct comprehensive evaluations with new auto-metric CoS for calculating the collaboration efficiency. Finally, our infrastructure can be deployed into real-world gaming scenarios in a customized VR version of CUISINEWORLD and adapted in existing broader Minecraft gaming domain. We hope our findings on LLMs and the new infrastructure for general-purpose scheduling and coordination can help shed light on how such skills can be obtained by learning from large language corpora.",
      "discovered_by": "hysts_dataset",
      "discovered_date": "2023-09-19",
      "status": "discovered",
      "insights": []
    }
  ]
}